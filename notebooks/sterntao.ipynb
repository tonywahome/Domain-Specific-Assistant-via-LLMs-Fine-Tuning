{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14858734,"datasetId":9504791,"databundleVersionId":15719691}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2de90957","cell_type":"markdown","source":"# Domain-Specific Financial Assistant via LLM Fine-Tuning\n\nThis notebook fine-tunes **Gemma-2B** using **QLoRA** (4-bit quantization) on the Financial-QA-10k dataset to create a domain-specific assistant for financial question answering.\n\n## Overview\n- **Model**: Google Gemma-2B\n- **Method**: QLoRA (4-bit quantization with LoRA adapters)\n- **Dataset**: Financial-QA-10k (3,000 samples from SEC 10-K filings)\n- **Format**: Alpaca instruction-response template\n- **Hardware**: Optimized for Kaggle T4/P100 GPUs\n\n## Sections\n1. Environment Setup\n2. Data Preprocessing\n3. Model Configuration\n4. Training\n5. Inference & Evaluation","metadata":{}},{"id":"26ffd30a","cell_type":"markdown","source":"---\n## 1. Environment Setup\n\nInstall required dependencies and set up the environment.","metadata":{}},{"id":"db57a385","cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:00:54.262247Z","iopub.execute_input":"2026-02-19T20:00:54.262540Z","iopub.status.idle":"2026-02-19T20:00:54.270866Z","shell.execute_reply.started":"2026-02-19T20:00:54.262514Z","shell.execute_reply":"2026-02-19T20:00:54.269925Z"}},"outputs":[],"execution_count":1},{"id":"2b40fe8d","cell_type":"code","source":"# Install required packages\n!pip install -q transformers>=4.40.0 \\\n    peft>=0.10.0 \\\n    datasets>=2.18.0 \\\n    accelerate>=0.27.0 \\\n    bitsandbytes>=0.49.2 \\\n    trl>=0.8.0 \\\n    sentencepiece>=0.2.0 \\\n    evaluate>=0.4.1 \\\n    rouge-score>=0.1.2 \\\n    scikit-learn>=1.4.0\n\nprint(\"✓ All packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:00:54.272715Z","iopub.execute_input":"2026-02-19T20:00:54.273056Z","iopub.status.idle":"2026-02-19T20:01:02.295576Z","shell.execute_reply.started":"2026-02-19T20:00:54.273016Z","shell.execute_reply":"2026-02-19T20:01:02.294696Z"}},"outputs":[{"name":"stdout","text":"✓ All packages installed successfully!\n","output_type":"stream"}],"execution_count":2},{"id":"a7cb304c","cell_type":"code","source":"# Import libraries\nimport os\nimport json\nimport re\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Suppress tokenizers parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nfrom datasets import Dataset, DatasetDict\nfrom trl import SFTTrainer\nimport evaluate\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:02.296960Z","iopub.execute_input":"2026-02-19T20:01:02.297269Z","iopub.status.idle":"2026-02-19T20:01:26.842388Z","shell.execute_reply.started":"2026-02-19T20:01:02.297212Z","shell.execute_reply":"2026-02-19T20:01:26.841573Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nVRAM: 15.64 GB\n","output_type":"stream"}],"execution_count":3},{"id":"c3058972","cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T21:04:15.965809Z","iopub.execute_input":"2026-02-19T21:04:15.966539Z","iopub.status.idle":"2026-02-19T21:04:16.104766Z","shell.execute_reply.started":"2026-02-19T21:04:15.966509Z","shell.execute_reply":"2026-02-19T21:04:16.104202Z"}},"outputs":[],"execution_count":35},{"id":"40258985","cell_type":"markdown","source":"---\n## 2. Data Preprocessing\n\nLoad and preprocess the Financial-QA-10k dataset into Alpaca format.","metadata":{}},{"id":"87c6c8ec","cell_type":"code","source":"# Configuration\nimport os\n\n# Auto-detect dataset path (Kaggle or local)\nif os.path.exists('/kaggle/input'):\n    # Kaggle environment - adjust this path based on your Kaggle dataset name\n    RAW_DATA_PATH = \"/kaggle/input/financial-qa-10k/Financial-QA-10k.csv\"\nelse:\n    # Local environment\n    RAW_DATA_PATH = \"../dataset/Financial-QA-10k.csv\"\n\nMODEL_NAME = \"google/gemma-2b\"\nMAX_SAMPLES = 3000\nMAX_SEQ_LENGTH = 512\nTRAIN_RATIO = 0.90\nVAL_RATIO = 0.05\nTEST_RATIO = 0.05\n\n# Alpaca prompt template\nALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}\"\"\"\n\nprint(f\"Configuration loaded successfully!\")\nprint(f\"Dataset path: {RAW_DATA_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.048507Z","iopub.execute_input":"2026-02-19T20:01:27.048752Z","iopub.status.idle":"2026-02-19T20:01:27.055190Z","shell.execute_reply.started":"2026-02-19T20:01:27.048727Z","shell.execute_reply":"2026-02-19T20:01:27.054306Z"}},"outputs":[{"name":"stdout","text":"Configuration loaded successfully!\nDataset path: /kaggle/input/financial-qa-10k/Financial-QA-10k.csv\n","output_type":"stream"}],"execution_count":5},{"id":"1d4773ba","cell_type":"code","source":"# Setup output paths - Auto-detect for Kaggle or local environment\nimport os\nfrom pathlib import Path\n\nif os.path.exists('/kaggle/working'):\n    # Running on Kaggle - save to /kaggle/working/\n    BASE_OUTPUT_DIR = \"/kaggle/working\"\n    MODELS_DIR = f\"{BASE_OUTPUT_DIR}/models/final/gemma-2b-financial-lora\"\n    OUTPUTS_DIR = f\"{BASE_OUTPUT_DIR}/outputs\"\n    RESULTS_DIR = f\"{BASE_OUTPUT_DIR}/outputs/results\"\n    ARCHIVE_DIR = f\"{BASE_OUTPUT_DIR}/models/final\"\n    print(\"Kaggle environment - outputs will be saved to /kaggle/working/\")\nelse:\n    # Running locally - save to industry folder\n    # Get the notebook's directory and go up one level to industry/\n    NOTEBOOK_DIR = Path.cwd()\n    INDUSTRY_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n    \n    BASE_OUTPUT_DIR = str(INDUSTRY_DIR)\n    MODELS_DIR = str(INDUSTRY_DIR / \"models\" / \"final\" / \"gemma-2b-financial-lora\")\n    OUTPUTS_DIR = str(INDUSTRY_DIR / \"outputs\")\n    RESULTS_DIR = str(INDUSTRY_DIR / \"outputs\" / \"results\")\n    ARCHIVE_DIR = str(INDUSTRY_DIR / \"models\" / \"final\")\n    print(f\"Local environment - outputs will be saved to {INDUSTRY_DIR}\")\n\nprint(f\"\\nOutput directories:\")\nprint(f\"  Models: {MODELS_DIR}\")\nprint(f\"  Outputs: {OUTPUTS_DIR}\")\nprint(f\"  Results: {RESULTS_DIR}\")\n\n# Create directories\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(OUTPUTS_DIR, exist_ok=True)\nos.makedirs(RESULTS_DIR, exist_ok=True)\nos.makedirs(ARCHIVE_DIR, exist_ok=True)\n\nprint(\"\\n✓ All output directories created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.055964Z","iopub.execute_input":"2026-02-19T20:01:27.056256Z","iopub.status.idle":"2026-02-19T20:01:27.074902Z","shell.execute_reply.started":"2026-02-19T20:01:27.056231Z","shell.execute_reply":"2026-02-19T20:01:27.074106Z"}},"outputs":[{"name":"stdout","text":"Kaggle environment - outputs will be saved to /kaggle/working/\n\nOutput directories:\n  Models: /kaggle/working/models/final/gemma-2b-financial-lora\n  Outputs: /kaggle/working/outputs\n  Results: /kaggle/working/outputs/results\n\n✓ All output directories created successfully!\n","output_type":"stream"}],"execution_count":6},{"id":"48e77d92","cell_type":"code","source":"# Helper functions\ndef normalize_text(text):\n    \"\"\"Normalize text by cleaning whitespace and standardizing formatting.\"\"\"\n    if pd.isna(text) or text is None:\n        return \"\"\n    \n    text = str(text).strip()\n    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n    text = re.sub(r'\\$\\s+', '$', text)  # Remove space after dollar sign\n    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n    \n    return text\n\n\ndef create_alpaca_format(row):\n    \"\"\"Convert a dataset row into Alpaca format.\"\"\"\n    return {\n        \"instruction\": normalize_text(row['question']),\n        \"input\": normalize_text(row['context']),\n        \"output\": normalize_text(row['answer']),\n        \"ticker\": row['ticker'],\n        \"filing\": row['filing']\n    }\n\n\ndef truncate_context(text, tokenizer, max_tokens=1500):\n    \"\"\"Truncate context to fit within token limit.\"\"\"\n    tokens = tokenizer.encode(text, add_special_tokens=False)\n    \n    if len(tokens) <= max_tokens:\n        return text\n    \n    truncated_tokens = tokens[:max_tokens]\n    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n    \n    # Try to end at sentence boundary\n    sentences = truncated_text.split('. ')\n    if len(sentences) > 1:\n        truncated_text = '. '.join(sentences[:-1]) + '.'\n    \n    return truncated_text\n\nprint(\"Helper functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.076007Z","iopub.execute_input":"2026-02-19T20:01:27.076316Z","iopub.status.idle":"2026-02-19T20:01:27.092374Z","shell.execute_reply.started":"2026-02-19T20:01:27.076292Z","shell.execute_reply":"2026-02-19T20:01:27.091421Z"}},"outputs":[{"name":"stdout","text":"Helper functions defined!\n","output_type":"stream"}],"execution_count":7},{"id":"5f907acc","cell_type":"code","source":"# Load dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(\"/kaggle/input/datasets/antonywambugu/financial-qa-10k/Financial-QA-10k.csv\")\nprint(f\"✓ Loaded {len(df)} examples\")\nprint(f\"\\nCompanies: {df['ticker'].unique().tolist()}\")\nprint(f\"\\nCompany distribution:\\n{df['ticker'].value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.093395Z","iopub.execute_input":"2026-02-19T20:01:27.093689Z","iopub.status.idle":"2026-02-19T20:01:27.196153Z","shell.execute_reply.started":"2026-02-19T20:01:27.093667Z","shell.execute_reply":"2026-02-19T20:01:27.195423Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n✓ Loaded 7000 examples\n\nCompanies: ['NVDA', 'AAPL', 'TSLA', 'LULU', 'PG', 'COST', 'ABNB', 'MSFT', 'BRK-A', 'META', 'AXP', 'PTON', 'SBUX', 'NKE', 'PLTR', 'AMZN', 'NFLX', 'GOOGL', 'ABBV', 'V', 'GME', 'AMC', 'CRM', 'LLY', 'AVGO', 'UNH', 'JNJ', 'HD', 'WMT', 'AMD', 'CVX', 'BAC', 'KO', 'T', 'AZO', 'CAT', 'SCHW', 'CMG', 'CB', 'CMCSA', 'CVS', 'DVA', 'DAL', 'DLTR', 'EBAY', 'EA', 'ENPH', 'EFX', 'ETSY', 'FDX', 'F', 'GRMN', 'GIS', 'GM', 'GILD', 'GS', 'HAS', 'HSY', 'HPE', 'HLT', 'HPQ', 'HUM', 'IBM', 'ICE', 'INTU', 'IRM', 'JPM', 'KR', 'LVS']\n\nCompany distribution:\nticker\nJNJ     200\nAAPL    100\nTSLA    100\nLULU    100\nNVDA    100\n       ... \nINTU    100\nIRM     100\nJPM     100\nKR      100\nLVS     100\nName: count, Length: 69, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"id":"1557a7d6","cell_type":"code","source":"# Sample data (stratified by company ticker)\nprint(f\"\\nSampling {MAX_SAMPLES} examples (stratified by ticker)...\")\n\nif len(df) > MAX_SAMPLES:\n    df_sampled = df.groupby('ticker', group_keys=False).apply(\n        lambda x: x.sample(frac=MAX_SAMPLES/len(df), random_state=42)\n    ).reset_index(drop=True)\n    \n    # Adjust to exact count\n    if len(df_sampled) < MAX_SAMPLES:\n        additional = df.drop(df_sampled.index).sample(\n            n=MAX_SAMPLES - len(df_sampled), random_state=42\n        )\n        df_sampled = pd.concat([df_sampled, additional]).reset_index(drop=True)\n    elif len(df_sampled) > MAX_SAMPLES:\n        df_sampled = df_sampled.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\nelse:\n    df_sampled = df.copy()\n\nprint(f\"✓ Selected {len(df_sampled)} examples\")\nprint(f\"\\nSampled distribution:\\n{df_sampled['ticker'].value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.196960Z","iopub.execute_input":"2026-02-19T20:01:27.197258Z","iopub.status.idle":"2026-02-19T20:01:27.245884Z","shell.execute_reply.started":"2026-02-19T20:01:27.197225Z","shell.execute_reply":"2026-02-19T20:01:27.245238Z"}},"outputs":[{"name":"stdout","text":"\nSampling 3000 examples (stratified by ticker)...\n✓ Selected 3000 examples\n\nSampled distribution:\nticker\nJNJ      86\nCMCSA    43\nTSLA     43\nABBV     43\nUNH      43\n         ..\nGRMN     42\nHLT      42\nDLTR     42\nGIS      42\nETSY     42\nName: count, Length: 69, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2908356001.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sampled = df.groupby('ticker', group_keys=False).apply(\n","output_type":"stream"}],"execution_count":9},{"id":"5de477cc","cell_type":"code","source":"# Load tokenizer\nprint(f\"\\nLoading tokenizer: {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Add padding token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:27.246852Z","iopub.execute_input":"2026-02-19T20:01:27.247117Z","iopub.status.idle":"2026-02-19T20:01:31.297135Z","shell.execute_reply.started":"2026-02-19T20:01:27.247094Z","shell.execute_reply":"2026-02-19T20:01:31.296438Z"}},"outputs":[{"name":"stdout","text":"\nLoading tokenizer: google/gemma-2b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e5936f5e3c4cddbbaf1df4abdd566c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce39d5a46c1b47578e49755c187fc232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab7216b0d0d422da1a8f3cf6758f610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a671a8117884d16bddcc8461453f5c1"}},"metadata":{}},{"name":"stdout","text":"✓ Tokenizer loaded (vocab size: 256000)\n","output_type":"stream"}],"execution_count":10},{"id":"47bd6113","cell_type":"code","source":"# Convert to Alpaca format and truncate if needed\nprint(\"\\nConverting to Alpaca format...\")\nformatted_data = []\ntruncated_count = 0\n\nfor idx, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Processing\"):\n    example = create_alpaca_format(row)\n    \n    # Check sequence length\n    full_prompt = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    token_count = len(tokenizer.encode(full_prompt, add_special_tokens=True))\n    \n    # Truncate if needed\n    if token_count > MAX_SEQ_LENGTH:\n        overhead = len(tokenizer.encode(\n            ALPACA_TEMPLATE.format(\n                instruction=example['instruction'],\n                input=\"\",\n                output=example['output']\n            ),\n            add_special_tokens=True\n        ))\n        \n        max_context_tokens = MAX_SEQ_LENGTH - overhead - 50\n        example['input'] = truncate_context(example['input'], tokenizer, max_context_tokens)\n        truncated_count += 1\n    \n    formatted_data.append(example)\n\nprint(f\"\\n✓ Formatted {len(formatted_data)} examples\")\nprint(f\"✓ Truncated {truncated_count} contexts to fit within {MAX_SEQ_LENGTH} tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:31.298021Z","iopub.execute_input":"2026-02-19T20:01:31.298282Z","iopub.status.idle":"2026-02-19T20:01:33.018646Z","shell.execute_reply.started":"2026-02-19T20:01:31.298245Z","shell.execute_reply":"2026-02-19T20:01:33.017661Z"}},"outputs":[{"name":"stdout","text":"\nConverting to Alpaca format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing:   0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9458e8aedb1741499270d387b9eccc35"}},"metadata":{}},{"name":"stdout","text":"\n✓ Formatted 3000 examples\n✓ Truncated 1 contexts to fit within 512 tokens\n","output_type":"stream"}],"execution_count":11},{"id":"1dca27d4","cell_type":"code","source":"# Analyze sequence lengths\nprint(\"\\nAnalyzing sequence lengths...\")\nlengths = []\n\nfor example in tqdm(formatted_data, desc=\"Measuring\"):\n    prompt = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n    lengths.append(len(tokens))\n\nprint(f\"\\n✓ Token length statistics:\")\nprint(f\"   Min: {min(lengths)}\")\nprint(f\"   Max: {max(lengths)}\")\nprint(f\"   Mean: {np.mean(lengths):.1f}\")\nprint(f\"   Median: {np.median(lengths):.1f}\")\nprint(f\"   95th percentile: {np.percentile(lengths, 95):.1f}\")\nprint(f\"   99th percentile: {np.percentile(lengths, 99):.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.019770Z","iopub.execute_input":"2026-02-19T20:01:33.020409Z","iopub.status.idle":"2026-02-19T20:01:33.854271Z","shell.execute_reply.started":"2026-02-19T20:01:33.020380Z","shell.execute_reply":"2026-02-19T20:01:33.853482Z"}},"outputs":[{"name":"stdout","text":"\nAnalyzing sequence lengths...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Measuring:   0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60414436d2554f46b0afe496e6b19c5d"}},"metadata":{}},{"name":"stdout","text":"\n✓ Token length statistics:\n   Min: 41\n   Max: 466\n   Mean: 136.1\n   Median: 128.0\n   95th percentile: 214.0\n   99th percentile: 283.0\n","output_type":"stream"}],"execution_count":12},{"id":"1e797c5a","cell_type":"code","source":"# Split data into train/val/test\nprint(f\"\\nSplitting data (train: {TRAIN_RATIO:.0%}, val: {VAL_RATIO:.0%}, test: {TEST_RATIO:.0%})...\")\n\n# First split: train + (val + test)\ntrain_data, temp_data = train_test_split(\n    formatted_data,\n    train_size=TRAIN_RATIO,\n    random_state=42,\n    stratify=[d['ticker'] for d in formatted_data]\n)\n\n# Second split: val and test\nval_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\nval_data, test_data = train_test_split(\n    temp_data,\n    train_size=val_ratio_adjusted,\n    random_state=42,\n    stratify=[d['ticker'] for d in temp_data]\n)\n\nprint(f\"✓ Train: {len(train_data)} examples\")\nprint(f\"✓ Validation: {len(val_data)} examples\")\nprint(f\"✓ Test: {len(test_data)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.858305Z","iopub.execute_input":"2026-02-19T20:01:33.859347Z","iopub.status.idle":"2026-02-19T20:01:33.874676Z","shell.execute_reply.started":"2026-02-19T20:01:33.859305Z","shell.execute_reply":"2026-02-19T20:01:33.873869Z"}},"outputs":[{"name":"stdout","text":"\nSplitting data (train: 90%, val: 5%, test: 5%)...\n✓ Train: 2700 examples\n✓ Validation: 150 examples\n✓ Test: 150 examples\n","output_type":"stream"}],"execution_count":13},{"id":"966bbd4a","cell_type":"code","source":"# Display sample examples\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE TRAINING EXAMPLE\")\nprint(\"=\"*60)\nsample = train_data[0]\nprint(f\"\\nTicker: {sample['ticker']}\")\nprint(f\"\\nInstruction:\\n{sample['instruction']}\")\nprint(f\"\\nInput (context):\\n{sample['input'][:300]}...\")  # Show first 300 chars\nprint(f\"\\nOutput:\\n{sample['output']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.875708Z","iopub.execute_input":"2026-02-19T20:01:33.876036Z","iopub.status.idle":"2026-02-19T20:01:33.882934Z","shell.execute_reply.started":"2026-02-19T20:01:33.876000Z","shell.execute_reply":"2026-02-19T20:01:33.882325Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSAMPLE TRAINING EXAMPLE\n============================================================\n\nTicker: SBUX\n\nInstruction:\nHow did the International segment's revenue performance in fiscal 2023 compare to fiscal 2022, despite COVID-19 impacts?\n\nInput (context):\nFor the International segment, despite COVID-19 pandemic-related headwinds in China in the first half of the year, revenue grew 8% in fiscal 2023 compared to fiscal 2022....\n\nOutput:\nThe International segment's revenue grew by 8% in fiscal 2023 compared to fiscal 2022, despite COVID-19 related challenges in China during the first half of the year.\n","output_type":"stream"}],"execution_count":14},{"id":"b698bc38","cell_type":"markdown","source":"---\n## 3. Model Configuration\n\nSet up QLoRA configuration and load the base model with 4-bit quantization.","metadata":{}},{"id":"769e4f6e","cell_type":"code","source":"# QLoRA Configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"QLoRA (4-bit quantization) configuration:\")\nprint(f\"  - Quantization type: NF4 (4-bit)\")\nprint(f\"  - Compute dtype: bfloat16\")\nprint(f\"  - Double quantization: Enabled\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.883936Z","iopub.execute_input":"2026-02-19T20:01:33.884262Z","iopub.status.idle":"2026-02-19T20:01:33.897741Z","shell.execute_reply.started":"2026-02-19T20:01:33.884219Z","shell.execute_reply":"2026-02-19T20:01:33.896937Z"}},"outputs":[{"name":"stdout","text":"QLoRA (4-bit quantization) configuration:\n  - Quantization type: NF4 (4-bit)\n  - Compute dtype: bfloat16\n  - Double quantization: Enabled\n","output_type":"stream"}],"execution_count":15},{"id":"e4ed1015","cell_type":"code","source":"# LoRA Configuration\npeft_config = LoraConfig(\n    r=8,  # LoRA rank\n    lora_alpha=16,  # LoRA scaling factor\n    lora_dropout=0.05,  # Dropout probability\n    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layer\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nprint(\"\\nLoRA configuration:\")\nprint(f\"  - Rank (r): {peft_config.r}\")\nprint(f\"  - Alpha: {peft_config.lora_alpha}\")\nprint(f\"  - Dropout: {peft_config.lora_dropout}\")\nprint(f\"  - Target modules: {peft_config.target_modules}\")\nprint(f\"  - Task type: {peft_config.task_type}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.898753Z","iopub.execute_input":"2026-02-19T20:01:33.899135Z","iopub.status.idle":"2026-02-19T20:01:33.912935Z","shell.execute_reply.started":"2026-02-19T20:01:33.899107Z","shell.execute_reply":"2026-02-19T20:01:33.912232Z"}},"outputs":[{"name":"stdout","text":"\nLoRA configuration:\n  - Rank (r): 8\n  - Alpha: 16\n  - Dropout: 0.05\n  - Target modules: {'v_proj', 'q_proj'}\n  - Task type: CAUSAL_LM\n","output_type":"stream"}],"execution_count":16},{"id":"4386466a","cell_type":"code","source":"# Load base model with quantization\nprint(f\"\\nLoading model: {MODEL_NAME}...\")\nprint(\"This may take a few minutes...\\n\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},           # Pin to first GPU to avoid DataParallel\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n)\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\nmodel.enable_input_require_grads()\n# Apply LoRA\nmodel = get_peft_model(model, peft_config)\n\nprint(\"✓ Model loaded and LoRA adapters applied!\")\n\n# Print trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"\\nTrainable params: {trainable_params:,} || \"\n          f\"All params: {all_param:,} || \"\n          f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n\nprint_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:33.913868Z","iopub.execute_input":"2026-02-19T20:01:33.914164Z","iopub.status.idle":"2026-02-19T20:01:57.917166Z","shell.execute_reply.started":"2026-02-19T20:01:33.914132Z","shell.execute_reply":"2026-02-19T20:01:57.916323Z"}},"outputs":[{"name":"stdout","text":"\nLoading model: google/gemma-2b...\nThis may take a few minutes...\n\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dcf01b6bcb74e29894366c193595830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6104a826a0346d2995cce2a727710e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcea60c130a94cd0a8bde6a0021fd70c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68fb05543a841f9b7dfefa40ee58183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"769ba174ee3249f28482b1186eb4e56f"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded and LoRA adapters applied!\n\nTrainable params: 921,600 || All params: 1,516,189,696 || Trainable%: 0.06%\n","output_type":"stream"}],"execution_count":17},{"id":"96408045","cell_type":"markdown","source":"---\n## 4. Training\n\nPrepare datasets and train the model using the SFTTrainer.","metadata":{}},{"id":"037f6853","cell_type":"code","source":"# Prepare datasets\ndef format_instruction(example):\n    \"\"\"Format example into Alpaca template for training.\"\"\"\n    text = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    return {\"text\": text}\n\n# Create HuggingFace datasets\ntrain_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in train_data])\nval_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in val_data])\ntest_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in test_data])\n\n# Apply formatting\ntrain_dataset = train_dataset.map(format_instruction)\nval_dataset = val_dataset.map(format_instruction)\ntest_dataset = test_dataset.map(format_instruction)\n\nprint(f\"✓ Prepared training dataset: {len(train_dataset)} examples\")\nprint(f\"✓ Prepared validation dataset: {len(val_dataset)} examples\")\nprint(f\"✓ Prepared test dataset: {len(test_dataset)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:57.918231Z","iopub.execute_input":"2026-02-19T20:01:57.918897Z","iopub.status.idle":"2026-02-19T20:01:58.222886Z","shell.execute_reply.started":"2026-02-19T20:01:57.918870Z","shell.execute_reply":"2026-02-19T20:01:58.222014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7236be5124349e0b475988faf97d10b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c78d3e10a9f4ffdbb88e4690172ab46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"810f7a5ae9e648f1a010982fd4f54ff5"}},"metadata":{}},{"name":"stdout","text":"✓ Prepared training dataset: 2700 examples\n✓ Prepared validation dataset: 150 examples\n✓ Prepared test dataset: 150 examples\n","output_type":"stream"}],"execution_count":18},{"id":"f218216e","cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUTS_DIR,\n    num_train_epochs=3,  # Increased from 2 to 3\n    per_device_train_batch_size=2,  # Increased from 1 to 2 if GPU allows\n    gradient_accumulation_steps=8,  # Adjusted to keep effective batch = 16\n    per_device_eval_batch_size=2,\n    learning_rate=1e-4,  # Reduced from 2e-4 - lower learning rate for stability\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=500,\n    eval_steps=500,\n    eval_strategy=\"steps\",  # Changed to \"steps\" to monitor training\n    save_strategy=\"steps\",\n    save_total_limit=2,  # Keep 2 checkpoints to compare\n    fp16=False,  # Enable fp16 for better training stability\n    bf16=False,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    optim=\"paged_adamw_8bit\",\n    max_grad_norm=1.0,  # CRITICAL FIX: Enable gradient clipping\n    report_to=\"none\",\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    load_best_model_at_end=True,  # Load best checkpoint at end\n)\n\nprint(\"Training configuration:\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Batch size (per device): {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")\nprint(f\"  - LR scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"  - Warmup steps: {training_args.warmup_steps}\")\nprint(f\"  - Max grad norm: {training_args.max_grad_norm}\")\nprint(f\"  - Optimizer: {training_args.optim}\")\nprint(f\"  - FP16: {training_args.fp16}\")\nprint(f\"  - Evaluation strategy: {training_args.eval_strategy}\")\nprint(f\"  - Gradient checkpointing: {training_args.gradient_checkpointing}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:01:58.223969Z","iopub.execute_input":"2026-02-19T20:01:58.224259Z","iopub.status.idle":"2026-02-19T20:02:07.174956Z","shell.execute_reply.started":"2026-02-19T20:01:58.224235Z","shell.execute_reply":"2026-02-19T20:02:07.174252Z"}},"outputs":[{"name":"stdout","text":"Training configuration:\n  - Epochs: 3\n  - Batch size (per device): 2\n  - Gradient accumulation steps: 8\n  - Effective batch size: 16\n  - Learning rate: 0.0001\n  - LR scheduler: SchedulerType.COSINE\n  - Warmup steps: 100\n  - Max grad norm: 1.0\n  - Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n  - FP16: False\n  - Evaluation strategy: IntervalStrategy.STEPS\n  - Gradient checkpointing: False\n","output_type":"stream"}],"execution_count":19},{"id":"7ed9e626","cell_type":"code","source":"\n# Check TrainingArguments configuration\nprint(\"--- TrainingArguments Precision Settings ---\")\nprint(f\"fp16 (Half Precision): {training_args.fp16}\")\nprint(f\"bf16 (Bfloat16): {training_args.bf16}\")\n\n# Check the actual model data type\nprint(\"\\n--- Model Hardware Precision ---\")\nprint(f\"Model primary dtype: {model.dtype}\")\n\n# If using QLoRA, check the 4-bit compute dtype\nif hasattr(model, \"config\") and hasattr(model.config, \"quantization_config\"):\n    compute_dtype = model.config.quantization_config.bnb_4bit_compute_dtype\n    print(f\"Quantization compute_dtype: {compute_dtype}\")\n\n# Verify CUDA compatibility for the selected precision\nif training_args.bf16:\n    cuda_supports_bf16 = torch.cuda.is_bf16_supported()\n    print(f\"\\n--- Hardware Compatibility ---\")\n    print(f\"GPU supports bf16: {cuda_supports_bf16}\")\n    if not cuda_supports_bf16:\n        print(\"WARNING: You are using bf16 on a GPU that does not support it (like Tesla T4). This will cause errors.\")\n\nif training_args.fp16:\n    print(f\"\\n--- Hardware Compatibility ---\")\n    print(f\"FP16 enabled: Compatible with most GPUs including T4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:02:07.175818Z","iopub.execute_input":"2026-02-19T20:02:07.176280Z","iopub.status.idle":"2026-02-19T20:02:07.183040Z","shell.execute_reply.started":"2026-02-19T20:02:07.176236Z","shell.execute_reply":"2026-02-19T20:02:07.182148Z"}},"outputs":[{"name":"stdout","text":"--- TrainingArguments Precision Settings ---\nfp16 (Half Precision): False\nbf16 (Bfloat16): False\n\n--- Model Hardware Precision ---\nModel primary dtype: torch.float32\nQuantization compute_dtype: torch.float16\n","output_type":"stream"}],"execution_count":20},{"id":"6480fc31","cell_type":"code","source":"# Clear GPU cache and reset accelerator state before training\nimport gc\nfrom accelerate import Accelerator\n\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Reset accelerator state if it exists (fixes retraining in notebooks)\ntry:\n    from accelerate.state import AcceleratorState\n    if AcceleratorState._shared_state != {}:\n        AcceleratorState._reset_state()\n        print(\"✓ Accelerator state reset\")\nexcept:\n    pass\n\n# Reinitialize trainer to ensure clean state\ntrainer = SFTTrainer(\n    model=model,                   \n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer,     \n    args=training_args,\n)\nprint(\"✓ Trainer reinitialized with clean accelerator state\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:02:07.184048Z","iopub.execute_input":"2026-02-19T20:02:07.184383Z","iopub.status.idle":"2026-02-19T20:02:19.331748Z","shell.execute_reply.started":"2026-02-19T20:02:07.184347Z","shell.execute_reply":"2026-02-19T20:02:19.330938Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5932cdc498f448458203c5bdbe954de5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a3c37ac4f8048b9a6f1101d6a488081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3affeb8c0eb84902b69727096f6826c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dead3bd6db542bdbeed2b2353aa868e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4baffa747c4d7db69a72c8814fcee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f22fa6a4bc4297992f5f303c0846b0"}},"metadata":{}},{"name":"stdout","text":"✓ Trainer reinitialized with clean accelerator state\n","output_type":"stream"}],"execution_count":21},{"id":"be7435cf","cell_type":"code","source":"\n# Start training\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\n# Train the model\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ TRAINING COMPLETE!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:02:19.332761Z","iopub.execute_input":"2026-02-19T20:02:19.333142Z","iopub.status.idle":"2026-02-19T20:48:07.376627Z","shell.execute_reply.started":"2026-02-19T20:02:19.333117Z","shell.execute_reply":"2026-02-19T20:48:07.376031Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSTARTING TRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [507/507 45:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.150336</td>\n      <td>1.068286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\n✓ TRAINING COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":22},{"id":"79c61142","cell_type":"code","source":"# Check training history to diagnose issues\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING DIAGNOSTICS\")\nprint(\"=\"*60)\n\nif hasattr(trainer.state, 'log_history') and len(trainer.state.log_history) > 0:\n    # Extract loss values\n    train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n    \n    print(f\"\\nTraining steps completed: {trainer.state.global_step}\")\n    print(f\"\\nTraining Loss:\")\n    print(f\"  - Initial: {train_losses[0]:.4f}\")\n    print(f\"  - Final: {train_losses[-1]:.4f}\")\n    print(f\"  - Change: {train_losses[-1] - train_losses[0]:.4f}\")\n    \n    if eval_losses:\n        print(f\"\\nValidation Loss:\")\n        print(f\"  - Best: {min(eval_losses):.4f}\")\n        print(f\"  - Final: {eval_losses[-1]:.4f}\")\n    \n    # Warning if loss didn't decrease\n    if len(train_losses) > 1 and train_losses[-1] >= train_losses[0]:\n        print(\"\\n  WARNING: Training loss did NOT decrease!\")\n        print(\"   This indicates training failed. Check hyperparameters.\")\n    else:\n        print(\"\\n✓ Training loss decreased successfully\")\nelse:\n    print(\" No training history available\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:48:07.377597Z","iopub.execute_input":"2026-02-19T20:48:07.377883Z","iopub.status.idle":"2026-02-19T20:48:07.385269Z","shell.execute_reply.started":"2026-02-19T20:48:07.377851Z","shell.execute_reply":"2026-02-19T20:48:07.384536Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING DIAGNOSTICS\n============================================================\n\nTraining steps completed: 507\n\nTraining Loss:\n  - Initial: 2.3174\n  - Final: 1.1503\n  - Change: -1.1670\n\nValidation Loss:\n  - Best: 1.0683\n  - Final: 1.0683\n\n✓ Training loss decreased successfully\n","output_type":"stream"}],"execution_count":23},{"id":"00ad4fe2","cell_type":"code","source":"# Save the final model\nprint(f\"Saving model to: {MODELS_DIR}\")\ntrainer.model.save_pretrained(MODELS_DIR)\ntokenizer.save_pretrained(MODELS_DIR)\n\nprint(f\"\\n✓ Model saved successfully!\")\nprint(f\"✓ Save location: {MODELS_DIR}\")\n\n# List saved files\nsaved_files = os.listdir(MODELS_DIR)\nprint(f\"\\nSaved files ({len(saved_files)}):\")\nfor file in saved_files[:10]:  # Show first 10 files\n    print(f\"  - {file}\")\n\nif len(saved_files) > 10:\n    print(f\"  ... and {len(saved_files) - 10} more files\")\n\nprint(\"\\nThe LoRA adapters have been saved locally.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:48:07.386311Z","iopub.execute_input":"2026-02-19T20:48:07.386624Z","iopub.status.idle":"2026-02-19T20:48:08.076962Z","shell.execute_reply.started":"2026-02-19T20:48:07.386589Z","shell.execute_reply":"2026-02-19T20:48:08.076234Z"}},"outputs":[{"name":"stdout","text":"Saving model to: /kaggle/working/models/final/gemma-2b-financial-lora\n\n✓ Model saved successfully!\n✓ Save location: /kaggle/working/models/final/gemma-2b-financial-lora\n\nSaved files (5):\n  - adapter_config.json\n  - adapter_model.safetensors\n  - tokenizer_config.json\n  - README.md\n  - tokenizer.json\n\nThe LoRA adapters have been saved locally.\n","output_type":"stream"}],"execution_count":24},{"id":"3e871911","cell_type":"markdown","source":"---\n## 5. Inference & Evaluation\n\nTest the fine-tuned model with sample questions and evaluate on the test set.","metadata":{}},{"id":"26deb09e","cell_type":"code","source":"# Inference function\ndef generate_response(instruction, input_context, max_new_tokens=256, temperature=0.7, top_p=0.9):\n    \"\"\"\n    Generate response for a given instruction and context.\n    \n    Args:\n        instruction: The question to answer\n        input_context: The context from 10-K filing\n        max_new_tokens: Maximum tokens to generate\n        temperature: Sampling temperature (higher = more creative)\n        top_p: Nucleus sampling parameter\n    \n    Returns:\n        Generated response text\n    \"\"\"\n    # Format prompt\n    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_context}\n\n### Response:\n\"\"\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # Decode\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the response part\n    response = full_output.split(\"### Response:\")[-1].strip()\n    \n    return response\n\nprint(\"✓ Inference function ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:48:08.078026Z","iopub.execute_input":"2026-02-19T20:48:08.078438Z","iopub.status.idle":"2026-02-19T20:48:08.084960Z","shell.execute_reply.started":"2026-02-19T20:48:08.078403Z","shell.execute_reply":"2026-02-19T20:48:08.084294Z"}},"outputs":[{"name":"stdout","text":"✓ Inference function ready!\n","output_type":"stream"}],"execution_count":25},{"id":"b5df75e1","cell_type":"code","source":"# Test with sample examples from test set\nprint(\"=\"*60)\nprint(\"INFERENCE EXAMPLES\")\nprint(\"=\"*60)\n\nnum_examples = 3\nfor i in range(min(num_examples, len(test_data))):\n    example = test_data[i]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"EXAMPLE {i+1} - {example['ticker']}\")\n    print(f\"{'='*60}\")\n    \n    print(f\"\\nQuestion:\\n{example['instruction']}\")\n    print(f\"\\nContext (excerpt):\\n{example['input'][:200]}...\")\n    \n    # Generate prediction\n    prediction = generate_response(\n        example['instruction'], \n        example['input'],\n        max_new_tokens=200,\n        temperature=0.7\n    )\n    \n    print(f\"\\n{'─'*60}\")\n    print(f\"MODEL PREDICTION:\\n{prediction}\")\n    print(f\"\\n{'─'*60}\")\n    print(f\"GROUND TRUTH:\\n{example['output']}\")\n    print(f\"{'─'*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:48:08.085878Z","iopub.execute_input":"2026-02-19T20:48:08.086162Z","iopub.status.idle":"2026-02-19T20:48:12.040614Z","shell.execute_reply.started":"2026-02-19T20:48:08.086132Z","shell.execute_reply":"2026-02-19T20:48:12.039945Z"}},"outputs":[{"name":"stdout","text":"============================================================\nINFERENCE EXAMPLES\n============================================================\n\n============================================================\nEXAMPLE 1 - AZO\n============================================================\n\nQuestion:\nWhat criteria did the independent audit use to assess the effectiveness of internal control over financial reporting at the company?\n\nContext (excerpt):\nThe independent registered public accounting firm conducted an audit based on the Internal Control-Integrated Framework issued by the Committee of Sponsoring Organizations of the Treadway Commission (...\n\n────────────────────────────────────────────────────────────\nMODEL PREDICTION:\nThe independent registered public accounting firm conducted an audit based on the 2013 framework issued by the Committee of Sponsoring Organizations of the Treadway Commission.\n\n────────────────────────────────────────────────────────────\nGROUND TRUTH:\nThe independent audit assessed the effectiveness of internal control over financial reporting based on criteria established in the Internal Control-Integrated Framework issued by the Committee of Sponsoring Organizations of the Treadway Commission (2013 framework). This framework guides the audit in evaluating whether the company has maintained effective controls over its financial reporting processes.\n────────────────────────────────────────────────────────────\n\n============================================================\nEXAMPLE 2 - GRMN\n============================================================\n\nQuestion:\nWhat are Garmin's reportable business segments as identified in their 2023 financial analysis?\n\nContext (excerpt):\nGarmin is organized in the five operating segments of fitness, outdoor, aviation, marine, and auto OEM. These operating segments represent our reportable segments....\n\n────────────────────────────────────────────────────────────\nMODEL PREDICTION:\nGarmin's reportable business segments are fitness, outdoor, aviation, marine, and auto OEM.\n\n────────────────────────────────────────────────────────────\nGROUND TRUTH:\nFitness, outdoor, aviation, marine, and auto OEM\n────────────────────────────────────────────────────────────\n\n============================================================\nEXAMPLE 3 - DVA\n============================================================\n\nQuestion:\nWhat percentage of U.S. dialysis patient service revenues in 2023 came from Medicare and Medicare Advantage plans?\n\nContext (excerpt):\nFor the year ended December 31, 2023, Medicare and Medicare Advantage plans accounted for 56% of U.S. dialysis patient service revenues....\n\n────────────────────────────────────────────────────────────\nMODEL PREDICTION:\n56%\n\n────────────────────────────────────────────────────────────\nGROUND TRUTH:\n56%\n────────────────────────────────────────────────────────────\n","output_type":"stream"}],"execution_count":26},{"id":"c6c10d18","cell_type":"code","source":"# Evaluate on test set\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING ON TEST SET\")\nprint(\"=\"*60)\n\n# Load ROUGE metric\nrouge = evaluate.load('rouge')\n\npredictions = []\nreferences = []\n\nprint(f\"\\nGenerating predictions for {len(test_data)} test examples...\\n\")\n\nfor example in tqdm(test_data[:100], desc=\"Evaluating\"):  # Evaluate on first 100 for speed\n    pred = generate_response(\n        example['instruction'], \n        example['input'],\n        max_new_tokens=200,\n        temperature=0.7\n    )\n    predictions.append(pred)\n    references.append(example['output'])\n\n# Calculate ROUGE scores\nrouge_scores = rouge.compute(predictions=predictions, references=references)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"\\nROUGE Scores (on {len(predictions)} test examples):\")\nprint(f\"  - ROUGE-1: {rouge_scores['rouge1']:.4f}\")\nprint(f\"  - ROUGE-2: {rouge_scores['rouge2']:.4f}\")\nprint(f\"  - ROUGE-L: {rouge_scores['rougeL']:.4f}\")\nprint(f\"  - ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:48:12.041524Z","iopub.execute_input":"2026-02-19T20:48:12.041856Z","iopub.status.idle":"2026-02-19T20:51:28.403449Z","shell.execute_reply.started":"2026-02-19T20:48:12.041830Z","shell.execute_reply":"2026-02-19T20:51:28.402571Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEVALUATING ON TEST SET\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922170495e7944f297ec57af2e4b02e3"}},"metadata":{}},{"name":"stdout","text":"\nGenerating predictions for 150 test examples...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38a15c14ff14765991fd4164f5309c3"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nEVALUATION RESULTS\n============================================================\n\nROUGE Scores (on 100 test examples):\n  - ROUGE-1: 0.6141\n  - ROUGE-2: 0.4463\n  - ROUGE-L: 0.5631\n  - ROUGE-Lsum: 0.5640\n","output_type":"stream"}],"execution_count":27},{"id":"cbafc588","cell_type":"code","source":"# Save evaluation results\nresults = {\n    \"model\": MODEL_NAME,\n    \"method\": \"QLoRA\",\n    \"lora_config\": {\n        \"r\": peft_config.r,\n        \"alpha\": peft_config.lora_alpha,\n        \"dropout\": peft_config.lora_dropout,\n        \"target_modules\": list(peft_config.target_modules) if isinstance(peft_config.target_modules, set) else peft_config.target_modules\n    },\n    \"training_examples\": len(train_data),\n    \"eval_examples\": len(predictions),\n    \"rouge_scores\": rouge_scores,\n    \"sample_predictions\": [\n        {\n            \"instruction\": test_data[i]['instruction'],\n            \"prediction\": predictions[i],\n            \"reference\": references[i]\n        }\n        for i in range(min(5, len(predictions)))\n    ]\n}\n\nresults_path = os.path.join(RESULTS_DIR, \"evaluation_results.json\")\n\nwith open(results_path, 'w', encoding='utf-8') as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\n✓ Evaluation results saved to: {results_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:59:09.582472Z","iopub.execute_input":"2026-02-19T20:59:09.582809Z","iopub.status.idle":"2026-02-19T20:59:09.590237Z","shell.execute_reply.started":"2026-02-19T20:59:09.582783Z","shell.execute_reply":"2026-02-19T20:59:09.589498Z"}},"outputs":[{"name":"stdout","text":"\n✓ Evaluation results saved to: /kaggle/working/outputs/results/evaluation_results.json\n","output_type":"stream"}],"execution_count":31},{"id":"f50f9ddb","cell_type":"markdown","source":"---\n## 6. Interactive Testing (Optional)\n\nTest the model with custom questions.","metadata":{}},{"id":"39453b4f","cell_type":"code","source":"# Interactive testing - modify these to test your own questions\ncustom_instruction = \"What was the company's total revenue?\"\ncustom_context = \"\"\"The company reported strong financial performance for fiscal year 2023. \nTotal revenue increased by 15% year-over-year to reach $26.97 billion. \nThis growth was primarily driven by increased demand for our products and services.\"\"\"\n\nprint(\"=\"*60)\nprint(\"CUSTOM QUESTION TEST\")\nprint(\"=\"*60)\nprint(f\"\\nQuestion: {custom_instruction}\")\nprint(f\"\\nContext: {custom_context}\")\n\nresponse = generate_response(custom_instruction, custom_context, max_new_tokens=150)\n\nprint(f\"\\nModel Response:\\n{response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T20:59:13.481186Z","iopub.execute_input":"2026-02-19T20:59:13.481712Z","iopub.status.idle":"2026-02-19T20:59:15.267387Z","shell.execute_reply.started":"2026-02-19T20:59:13.481681Z","shell.execute_reply":"2026-02-19T20:59:15.266623Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCUSTOM QUESTION TEST\n============================================================\n\nQuestion: What was the company's total revenue?\n\nContext: The company reported strong financial performance for fiscal year 2023. \nTotal revenue increased by 15% year-over-year to reach $26.97 billion. \nThis growth was primarily driven by increased demand for our products and services.\n\nModel Response:\nThe company's total revenue increased by 15% year-over-year to reach $26.97 billion.\n","output_type":"stream"}],"execution_count":32},{"id":"287ce3b0","cell_type":"markdown","source":"---\n## 7. Export Model (Optional)\n\nUpload to HuggingFace Hub or download locally.","metadata":{}},{"id":"735db972","cell_type":"code","source":"# Push to HuggingFace Hub\n# Uncomment and configure if you want to share your model\n\nfrom huggingface_hub import HfApi\n \nmodel_id = \"Antonomics/gemma-2b-financial-qa-lora\"\ntrainer.model.push_to_hub(model_id)\ntokenizer.push_to_hub(model_id)\n \nprint(f\"✓ Model pushed to HuggingFace Hub: {model_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T21:04:27.602823Z","iopub.execute_input":"2026-02-19T21:04:27.603405Z","iopub.status.idle":"2026-02-19T21:04:31.878021Z","shell.execute_reply.started":"2026-02-19T21:04:27.603374Z","shell.execute_reply":"2026-02-19T21:04:31.877186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d12b0d15aab4e06bec1bdf7e4f53d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81719212b5834064b781abab695b1b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c70771742a246849a2c6fe15452f959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8861a1b71b164082b9c708eb7f210f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94a41ceedd646a297fad794eb103bb1"}},"metadata":{}},{"name":"stdout","text":"✓ Model pushed to HuggingFace Hub: Antonomics/gemma-2b-financial-qa-lora\n","output_type":"stream"}],"execution_count":36},{"id":"44a2ea8d","cell_type":"code","source":"# Create a downloadable archive (for local use)\nimport shutil\n\narchive_path = os.path.join(ARCHIVE_DIR, \"gemma-2b-financial-qa-model\")\nif os.path.exists(archive_path + \".zip\"):\n    os.remove(archive_path + \".zip\")\n\nshutil.make_archive(archive_path, 'zip', MODELS_DIR)\nprint(f\"✓ Model archived to: {archive_path}.zip\")\nprint(\"\\nThe model archive has been saved locally.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T21:05:22.550504Z","iopub.execute_input":"2026-02-19T21:05:22.551214Z","iopub.status.idle":"2026-02-19T21:05:24.143093Z","shell.execute_reply.started":"2026-02-19T21:05:22.551181Z","shell.execute_reply":"2026-02-19T21:05:24.142387Z"}},"outputs":[{"name":"stdout","text":"✓ Model archived to: /kaggle/working/models/final/gemma-2b-financial-qa-model.zip\n\nThe model archive has been saved locally.\n","output_type":"stream"}],"execution_count":37},{"id":"1570eb58","cell_type":"markdown","source":"---\n## Troubleshooting Guide\n\n### If You See Repetitive Text (Model Collapse):\n\n**Symptoms**: Model outputs \"TheTheThe...\" or \"$$$...\" repeatedly\n\n**Common Causes & Fixes**:\n1. **Gradient clipping disabled** → Set `max_grad_norm=1.0` ✓ (Fixed above)\n2. **Learning rate too high** → Reduced to `1e-4` from `2e-4` ✓ (Fixed above)\n3. **Not enough training** → Increased to 3 epochs ✓ (Fixed above)\n4. **No monitoring** → Enabled `eval_strategy=\"steps\"` ✓ (Fixed above)\n\n### What to Check After Retraining:\n- Training loss should **decrease** (check cell 27 output)\n- Validation loss should **decrease** (monitored every 500 steps)\n- First inference examples should show **coherent text**, not repetition\n\n### If Problems Persist:\n- Increase `num_train_epochs` to 4-5\n- Try `learning_rate=5e-5` (even lower)\n- Increase `max_seq_length` to 512 (more context)\n- Check that your GPU has enough VRAM","metadata":{}},{"id":"c0b57148","cell_type":"markdown","source":"---\n## Summary\n\nYou have successfully:\n1. ✓ Preprocessed 5,000 financial Q&A examples into Alpaca format\n2. ✓ Fine-tuned Gemma-2B using QLoRA (4-bit quantization)\n3. ✓ Evaluated the model on test data using ROUGE metrics\n4. ✓ Saved the fine-tuned LoRA adapters\n\n**Next Steps:**\n- Experiment with different hyperparameters (learning rate, LoRA rank, etc.)\n- Try fine-tuning on the full dataset (7,000+ examples)\n- Test with real-world financial questions\n- Integrate into a RAG (Retrieval-Augmented Generation) pipeline\n- Deploy as an API using FastAPI or Gradio\n\n**Model Usage:**\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load base model and LoRA adapters\nbase_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\nmodel = PeftModel.from_pretrained(base_model, \"path/to/lora/adapters\")\ntokenizer = AutoTokenizer.from_pretrained(\"path/to/lora/adapters\")\n```","metadata":{}}]}