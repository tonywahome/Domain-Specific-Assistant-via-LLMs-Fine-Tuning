{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-Specific Financial Assistant via LLM Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes **Gemma-2B** using **QLoRA** (4-bit quantization) on the Financial-QA-10k dataset to create a domain-specific assistant for financial question answering.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Google Gemma-2B\n",
    "- **Method**: QLoRA (4-bit quantization with LoRA adapters)\n",
    "- **Dataset**: Financial-QA-10k (3,000 samples from SEC 10-K filings)\n",
    "- **Format**: Alpaca instruction-response template\n",
    "- **Hardware**: Optimized for Kaggle T4/P100 GPUs\n",
    "\n",
    "## Sections\n",
    "1. Environment Setup\n",
    "2. Data Preprocessing\n",
    "3. Model Configuration\n",
    "4. Training\n",
    "5. Inference & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Install required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:28:42.426539Z",
     "iopub.status.busy": "2026-02-18T04:28:42.426231Z",
     "iopub.status.idle": "2026-02-18T04:28:42.433120Z",
     "shell.execute_reply": "2026-02-18T04:28:42.432418Z",
     "shell.execute_reply.started": "2026-02-18T04:28:42.426512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:28:42.445071Z",
     "iopub.status.busy": "2026-02-18T04:28:42.444842Z",
     "iopub.status.idle": "2026-02-18T04:28:49.771392Z",
     "shell.execute_reply": "2026-02-18T04:28:49.770505Z",
     "shell.execute_reply.started": "2026-02-18T04:28:42.445049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.40.0 \\\n",
    "    peft>=0.10.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    accelerate>=0.27.0 \\\n",
    "    bitsandbytes>=0.43.0 \\\n",
    "    trl>=0.8.0 \\\n",
    "    sentencepiece>=0.2.0 \\\n",
    "    evaluate>=0.4.1 \\\n",
    "    rouge-score>=0.1.2 \\\n",
    "    scikit-learn>=1.4.0\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:28:49.773455Z",
     "iopub.status.busy": "2026-02-18T04:28:49.773159Z",
     "iopub.status.idle": "2026-02-18T04:29:24.646414Z",
     "shell.execute_reply": "2026-02-18T04:29:24.645677Z",
     "shell.execute_reply.started": "2026-02-18T04:28:49.773400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:24.647863Z",
     "iopub.status.busy": "2026-02-18T04:29:24.647621Z",
     "iopub.status.idle": "2026-02-18T04:29:24.813612Z",
     "shell.execute_reply": "2026-02-18T04:29:24.812944Z",
     "shell.execute_reply.started": "2026-02-18T04:29:24.647841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optional: Login to Hugging Face (for model uploads)\n",
    "# Set HUGGINGFACE_TOKEN environment variable or use notebook secrets\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "if 'HUGGINGFACE_TOKEN' in os.environ:\n",
    "    login(token=os.environ['HUGGINGFACE_TOKEN'])\n",
    "    print(\"✓ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"⚠ HUGGINGFACE_TOKEN not set - skipping HF login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "Load and preprocess the Financial-QA-10k dataset into Alpaca format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:24.816415Z",
     "iopub.status.busy": "2026-02-18T04:29:24.815773Z",
     "iopub.status.idle": "2026-02-18T04:29:25.271857Z",
     "shell.execute_reply": "2026-02-18T04:29:25.271185Z",
     "shell.execute_reply.started": "2026-02-18T04:29:24.816350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RAW_DATA_PATH = \"../dataset/Financial-QA-10k.csv\"  \n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "MAX_SAMPLES = 7000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "TRAIN_RATIO = 0.90\n",
    "VAL_RATIO = 0.05\n",
    "TEST_RATIO = 0.05\n",
    "\n",
    "# Alpaca prompt template\n",
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:25.273210Z",
     "iopub.status.busy": "2026-02-18T04:29:25.272735Z",
     "iopub.status.idle": "2026-02-18T04:29:25.284686Z",
     "shell.execute_reply": "2026-02-18T04:29:25.283976Z",
     "shell.execute_reply.started": "2026-02-18T04:29:25.273175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by cleaning whitespace and standardizing formatting.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
    "    text = re.sub(r'\\$\\s+', '$', text)  # Remove space after dollar sign\n",
    "    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def create_alpaca_format(row):\n",
    "    \"\"\"Convert a dataset row into Alpaca format.\"\"\"\n",
    "    return {\n",
    "        \"instruction\": normalize_text(row['question']),\n",
    "        \"input\": normalize_text(row['context']),\n",
    "        \"output\": normalize_text(row['answer']),\n",
    "        \"ticker\": row['ticker'],\n",
    "        \"filing\": row['filing']\n",
    "    }\n",
    "\n",
    "\n",
    "def truncate_context(text, tokenizer, max_tokens=1500):\n",
    "    \"\"\"Truncate context to fit within token limit.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    \n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Try to end at sentence boundary\n",
    "    sentences = truncated_text.split('. ')\n",
    "    if len(sentences) > 1:\n",
    "        truncated_text = '. '.join(sentences[:-1]) + '.'\n",
    "    \n",
    "    return truncated_text\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:25.285723Z",
     "iopub.status.busy": "2026-02-18T04:29:25.285454Z",
     "iopub.status.idle": "2026-02-18T04:29:25.390734Z",
     "shell.execute_reply": "2026-02-18T04:29:25.390122Z",
     "shell.execute_reply.started": "2026-02-18T04:29:25.285699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "✓ Loaded 7000 examples\n",
      "\n",
      "Companies: ['NVDA', 'AAPL', 'TSLA', 'LULU', 'PG', 'COST', 'ABNB', 'MSFT', 'BRK-A', 'META', 'AXP', 'PTON', 'SBUX', 'NKE', 'PLTR', 'AMZN', 'NFLX', 'GOOGL', 'ABBV', 'V', 'GME', 'AMC', 'CRM', 'LLY', 'AVGO', 'UNH', 'JNJ', 'HD', 'WMT', 'AMD', 'CVX', 'BAC', 'KO', 'T', 'AZO', 'CAT', 'SCHW', 'CMG', 'CB', 'CMCSA', 'CVS', 'DVA', 'DAL', 'DLTR', 'EBAY', 'EA', 'ENPH', 'EFX', 'ETSY', 'FDX', 'F', 'GRMN', 'GIS', 'GM', 'GILD', 'GS', 'HAS', 'HSY', 'HPE', 'HLT', 'HPQ', 'HUM', 'IBM', 'ICE', 'INTU', 'IRM', 'JPM', 'KR', 'LVS']\n",
      "\n",
      "Company distribution:\n",
      "ticker\n",
      "JNJ     200\n",
      "AAPL    100\n",
      "TSLA    100\n",
      "LULU    100\n",
      "NVDA    100\n",
      "       ... \n",
      "INTU    100\n",
      "IRM     100\n",
      "JPM     100\n",
      "KR      100\n",
      "LVS     100\n",
      "Name: count, Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "print(f\"✓ Loaded {len(df)} examples\")\n",
    "print(f\"\\nCompanies: {df['ticker'].unique().tolist()}\")\n",
    "print(f\"\\nCompany distribution:\\n{df['ticker'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:25.391753Z",
     "iopub.status.busy": "2026-02-18T04:29:25.391554Z",
     "iopub.status.idle": "2026-02-18T04:29:25.446181Z",
     "shell.execute_reply": "2026-02-18T04:29:25.445414Z",
     "shell.execute_reply.started": "2026-02-18T04:29:25.391733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling 7000 examples (stratified by ticker)...\n",
      "✓ Selected 7000 examples\n",
      "\n",
      "Sampled distribution:\n",
      "ticker\n",
      "JNJ     200\n",
      "AAPL    100\n",
      "TSLA    100\n",
      "LULU    100\n",
      "NVDA    100\n",
      "       ... \n",
      "INTU    100\n",
      "IRM     100\n",
      "JPM     100\n",
      "KR      100\n",
      "LVS     100\n",
      "Name: count, Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample data (stratified by company ticker)\n",
    "print(f\"\\nSampling {MAX_SAMPLES} examples (stratified by ticker)...\")\n",
    "\n",
    "if len(df) > MAX_SAMPLES:\n",
    "    df_sampled = df.groupby('ticker', group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=MAX_SAMPLES/len(df), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Adjust to exact count\n",
    "    if len(df_sampled) < MAX_SAMPLES:\n",
    "        additional = df.drop(df_sampled.index).sample(\n",
    "            n=MAX_SAMPLES - len(df_sampled), random_state=42\n",
    "        )\n",
    "        df_sampled = pd.concat([df_sampled, additional]).reset_index(drop=True)\n",
    "    elif len(df_sampled) > MAX_SAMPLES:\n",
    "        df_sampled = df_sampled.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_sampled = df.copy()\n",
    "\n",
    "print(f\"✓ Selected {len(df_sampled)} examples\")\n",
    "print(f\"\\nSampled distribution:\\n{df_sampled['ticker'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:25.447598Z",
     "iopub.status.busy": "2026-02-18T04:29:25.447273Z",
     "iopub.status.idle": "2026-02-18T04:29:28.065105Z",
     "shell.execute_reply": "2026-02-18T04:29:28.064411Z",
     "shell.execute_reply.started": "2026-02-18T04:29:25.447567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer: google/gemma-2b...\n",
      "✓ Tokenizer loaded (vocab size: 256000)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:28.066432Z",
     "iopub.status.busy": "2026-02-18T04:29:28.066134Z",
     "iopub.status.idle": "2026-02-18T04:29:29.236786Z",
     "shell.execute_reply": "2026-02-18T04:29:29.235924Z",
     "shell.execute_reply.started": "2026-02-18T04:29:28.066394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting to Alpaca format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1492bdcb2e5e40e5a9bb218d0df81ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Formatted 7000 examples\n",
      "✓ Truncated 147 contexts to fit within 256 tokens\n"
     ]
    }
   ],
   "source": [
    "# Convert to Alpaca format and truncate if needed\n",
    "print(\"\\nConverting to Alpaca format...\")\n",
    "formatted_data = []\n",
    "truncated_count = 0\n",
    "\n",
    "for idx, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Processing\"):\n",
    "    example = create_alpaca_format(row)\n",
    "    \n",
    "    # Check sequence length\n",
    "    full_prompt = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    token_count = len(tokenizer.encode(full_prompt, add_special_tokens=True))\n",
    "    \n",
    "    # Truncate if needed\n",
    "    if token_count > MAX_SEQ_LENGTH:\n",
    "        overhead = len(tokenizer.encode(\n",
    "            ALPACA_TEMPLATE.format(\n",
    "                instruction=example['instruction'],\n",
    "                input=\"\",\n",
    "                output=example['output']\n",
    "            ),\n",
    "            add_special_tokens=True\n",
    "        ))\n",
    "        \n",
    "        max_context_tokens = MAX_SEQ_LENGTH - overhead - 50\n",
    "        example['input'] = truncate_context(example['input'], tokenizer, max_context_tokens)\n",
    "        truncated_count += 1\n",
    "    \n",
    "    formatted_data.append(example)\n",
    "\n",
    "print(f\"\\n✓ Formatted {len(formatted_data)} examples\")\n",
    "print(f\"✓ Truncated {truncated_count} contexts to fit within {MAX_SEQ_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.239495Z",
     "iopub.status.busy": "2026-02-18T04:29:29.239239Z",
     "iopub.status.idle": "2026-02-18T04:29:29.787652Z",
     "shell.execute_reply": "2026-02-18T04:29:29.786764Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.239472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing sequence lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e719b60ac86a4472a1625f2d1ac7f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Measuring:   0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Token length statistics:\n",
      "   Min: 41\n",
      "   Max: 390\n",
      "   Mean: 134.5\n",
      "   Median: 128.0\n",
      "   95th percentile: 207.0\n",
      "   99th percentile: 238.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze sequence lengths\n",
    "print(\"\\nAnalyzing sequence lengths...\")\n",
    "lengths = []\n",
    "\n",
    "for example in tqdm(formatted_data, desc=\"Measuring\"):\n",
    "    prompt = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    lengths.append(len(tokens))\n",
    "\n",
    "print(f\"\\n✓ Token length statistics:\")\n",
    "print(f\"   Min: {min(lengths)}\")\n",
    "print(f\"   Max: {max(lengths)}\")\n",
    "print(f\"   Mean: {np.mean(lengths):.1f}\")\n",
    "print(f\"   Median: {np.median(lengths):.1f}\")\n",
    "print(f\"   95th percentile: {np.percentile(lengths, 95):.1f}\")\n",
    "print(f\"   99th percentile: {np.percentile(lengths, 99):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.789533Z",
     "iopub.status.busy": "2026-02-18T04:29:29.788845Z",
     "iopub.status.idle": "2026-02-18T04:29:29.802491Z",
     "shell.execute_reply": "2026-02-18T04:29:29.801827Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.789506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data (train: 90%, val: 5%, test: 5%)...\n",
      "✓ Train: 6300 examples\n",
      "✓ Validation: 350 examples\n",
      "✓ Test: 350 examples\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test\n",
    "print(f\"\\nSplitting data (train: {TRAIN_RATIO:.0%}, val: {VAL_RATIO:.0%}, test: {TEST_RATIO:.0%})...\")\n",
    "\n",
    "# First split: train + (val + test)\n",
    "train_data, temp_data = train_test_split(\n",
    "    formatted_data,\n",
    "    train_size=TRAIN_RATIO,\n",
    "    random_state=42,\n",
    "    stratify=[d['ticker'] for d in formatted_data]\n",
    ")\n",
    "\n",
    "# Second split: val and test\n",
    "val_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    train_size=val_ratio_adjusted,\n",
    "    random_state=42,\n",
    "    stratify=[d['ticker'] for d in temp_data]\n",
    ")\n",
    "\n",
    "print(f\"✓ Train: {len(train_data)} examples\")\n",
    "print(f\"✓ Validation: {len(val_data)} examples\")\n",
    "print(f\"✓ Test: {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.803677Z",
     "iopub.status.busy": "2026-02-18T04:29:29.803412Z",
     "iopub.status.idle": "2026-02-18T04:29:29.810041Z",
     "shell.execute_reply": "2026-02-18T04:29:29.809285Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.803652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE TRAINING EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Ticker: DAL\n",
      "\n",
      "Instruction:\n",
      "What is the net amount of Delta Air Lines' total assets in 2023 according to their financial statements?\n",
      "\n",
      "Input (context):\n",
      "Total assets for Delta Air Lines in 2023 are reported as $73,644 million in the financial statements....\n",
      "\n",
      "Output:\n",
      "$73,644 million\n"
     ]
    }
   ],
   "source": [
    "# Display sample examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "sample = train_data[0]\n",
    "print(f\"\\nTicker: {sample['ticker']}\")\n",
    "print(f\"\\nInstruction:\\n{sample['instruction']}\")\n",
    "print(f\"\\nInput (context):\\n{sample['input'][:300]}...\")  # Show first 300 chars\n",
    "print(f\"\\nOutput:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Configuration\n",
    "\n",
    "Set up QLoRA configuration and load the base model with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.811523Z",
     "iopub.status.busy": "2026-02-18T04:29:29.811062Z",
     "iopub.status.idle": "2026-02-18T04:29:29.825538Z",
     "shell.execute_reply": "2026-02-18T04:29:29.824702Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.811497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA (4-bit quantization) configuration:\n",
      "  - Quantization type: NF4 (4-bit)\n",
      "  - Compute dtype: bfloat16\n",
      "  - Double quantization: Enabled\n"
     ]
    }
   ],
   "source": [
    "# QLoRA Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"QLoRA (4-bit quantization) configuration:\")\n",
    "print(f\"  - Quantization type: NF4 (4-bit)\")\n",
    "print(f\"  - Compute dtype: bfloat16\")\n",
    "print(f\"  - Double quantization: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.826790Z",
     "iopub.status.busy": "2026-02-18T04:29:29.826502Z",
     "iopub.status.idle": "2026-02-18T04:29:29.837849Z",
     "shell.execute_reply": "2026-02-18T04:29:29.837171Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.826764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA configuration:\n",
      "  - Rank (r): 8\n",
      "  - Alpha: 16\n",
      "  - Dropout: 0.05\n",
      "  - Target modules: {'q_proj', 'v_proj'}\n",
      "  - Task type: CAUSAL_LM\n"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,  # LoRA scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layer\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"\\nLoRA configuration:\")\n",
    "print(f\"  - Rank (r): {peft_config.r}\")\n",
    "print(f\"  - Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  - Dropout: {peft_config.lora_dropout}\")\n",
    "print(f\"  - Target modules: {peft_config.target_modules}\")\n",
    "print(f\"  - Task type: {peft_config.task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:29.839224Z",
     "iopub.status.busy": "2026-02-18T04:29:29.838918Z",
     "iopub.status.idle": "2026-02-18T04:29:57.496569Z",
     "shell.execute_reply": "2026-02-18T04:29:57.495881Z",
     "shell.execute_reply.started": "2026-02-18T04:29:29.839199Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: google/gemma-2b...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252e2d7be1be4dfca337f0bf72434dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded and LoRA adapters applied!\n",
      "\n",
      "Trainable params: 921,600 || All params: 1,516,189,696 || Trainable%: 0.06%\n"
     ]
    }
   ],
   "source": [
    "# Load base model with quantization\n",
    "print(f\"\\nLoading model: {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},           # Pin to first GPU to avoid DataParallel\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.enable_input_require_grads()\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"✓ Model loaded and LoRA adapters applied!\")\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"\\nTrainable params: {trainable_params:,} || \"\n",
    "          f\"All params: {all_param:,} || \"\n",
    "          f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training\n",
    "\n",
    "Prepare datasets and train the model using the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:57.498337Z",
     "iopub.status.busy": "2026-02-18T04:29:57.497616Z",
     "iopub.status.idle": "2026-02-18T04:29:57.691619Z",
     "shell.execute_reply": "2026-02-18T04:29:57.690817Z",
     "shell.execute_reply.started": "2026-02-18T04:29:57.498295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9a0a61fd7d481587a6ebda6f82cd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0510ba661e6540538d46d54ae10b5ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9353ad31ed424e1094c421226e7fb79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared training dataset: 6300 examples\n",
      "✓ Prepared validation dataset: 350 examples\n",
      "✓ Prepared test dataset: 350 examples\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format example into Alpaca template for training.\"\"\"\n",
    "    text = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in train_data])\n",
    "val_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in val_data])\n",
    "test_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in test_data])\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_instruction)\n",
    "val_dataset = val_dataset.map(format_instruction)\n",
    "test_dataset = test_dataset.map(format_instruction)\n",
    "\n",
    "print(f\"✓ Prepared training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"✓ Prepared validation dataset: {len(val_dataset)} examples\")\n",
    "print(f\"✓ Prepared test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:29:57.692854Z",
     "iopub.status.busy": "2026-02-18T04:29:57.692614Z",
     "iopub.status.idle": "2026-02-18T04:30:06.285163Z",
     "shell.execute_reply": "2026-02-18T04:30:06.284349Z",
     "shell.execute_reply.started": "2026-02-18T04:29:57.692831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  - Epochs: 3\n",
      "  - Batch size (per device): 2\n",
      "  - Gradient accumulation steps: 8\n",
      "  - Effective batch size: 16\n",
      "  - Learning rate: 0.0001\n",
      "  - LR scheduler: SchedulerType.COSINE\n",
      "  - Warmup steps: 100\n",
      "  - Max grad norm: 1.0\n",
      "  - Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n",
      "  - FP16: False\n",
      "  - Evaluation strategy: IntervalStrategy.STEPS\n",
      "  - Gradient checkpointing: False\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "output_dir = \"../outputs/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,  # Increased from 2 to 3\n",
    "    per_device_train_batch_size=2,  # Increased from 1 to 2 if GPU allows\n",
    "    gradient_accumulation_steps=8,  # Adjusted to keep effective batch = 16\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,  # Reduced from 2e-4 - lower learning rate for stability\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",  # Changed to \"steps\" to monitor training\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,  # Keep 2 checkpoints to compare\n",
    "    fp16=False,  # Enable fp16 for better training stability\n",
    "    bf16=False,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=1.0,  # CRITICAL FIX: Enable gradient clipping\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # Load best checkpoint at end\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"  - Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  - Max grad norm: {training_args.max_grad_norm}\")\n",
    "print(f\"  - Optimizer: {training_args.optim}\")\n",
    "print(f\"  - FP16: {training_args.fp16}\")\n",
    "print(f\"  - Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  - Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:30:08.168379Z",
     "iopub.status.busy": "2026-02-18T04:30:08.168068Z",
     "iopub.status.idle": "2026-02-18T04:30:08.174169Z",
     "shell.execute_reply": "2026-02-18T04:30:08.173521Z",
     "shell.execute_reply.started": "2026-02-18T04:30:08.168337Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TrainingArguments Precision Settings ---\n",
      "fp16 (Half Precision): False\n",
      "bf16 (Bfloat16): False\n",
      "\n",
      "--- Model Hardware Precision ---\n",
      "Model primary dtype: torch.float32\n",
      "Quantization compute_dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check TrainingArguments configuration\n",
    "print(\"--- TrainingArguments Precision Settings ---\")\n",
    "print(f\"fp16 (Half Precision): {training_args.fp16}\")\n",
    "print(f\"bf16 (Bfloat16): {training_args.bf16}\")\n",
    "\n",
    "# Check the actual model data type\n",
    "print(\"\\n--- Model Hardware Precision ---\")\n",
    "print(f\"Model primary dtype: {model.dtype}\")\n",
    "\n",
    "# If using QLoRA, check the 4-bit compute dtype\n",
    "if hasattr(model, \"config\") and hasattr(model.config, \"quantization_config\"):\n",
    "    compute_dtype = model.config.quantization_config.bnb_4bit_compute_dtype\n",
    "    print(f\"Quantization compute_dtype: {compute_dtype}\")\n",
    "\n",
    "# Verify CUDA compatibility for the selected precision\n",
    "if training_args.bf16:\n",
    "    cuda_supports_bf16 = torch.cuda.is_bf16_supported()\n",
    "    print(f\"\\n--- Hardware Compatibility ---\")\n",
    "    print(f\"GPU supports bf16: {cuda_supports_bf16}\")\n",
    "    if not cuda_supports_bf16:\n",
    "        print(\"WARNING: You are using bf16 on a GPU that does not support it (like Tesla T4). This will cause errors.\")\n",
    "\n",
    "if training_args.fp16:\n",
    "    print(f\"\\n--- Hardware Compatibility ---\")\n",
    "    print(f\"FP16 enabled: Compatible with most GPUs including T4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:30:08.175411Z",
     "iopub.status.busy": "2026-02-18T04:30:08.175112Z",
     "iopub.status.idle": "2026-02-18T04:30:08.632226Z",
     "shell.execute_reply": "2026-02-18T04:30:08.631585Z",
     "shell.execute_reply.started": "2026-02-18T04:30:08.175373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973949b7e70b4b8b85100bbdf8e999e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb965550af3484e91e10bb04054e042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498e609246284198a722f38a30ff6157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427a846bc10c4e9abbfee666752831bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ec1b94c728487f9607a7523a3d00a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289fd7a43b444a7d92191ed7d1af9e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer reinitialized with clean accelerator state\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache and reset accelerator state before training\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset accelerator state if it exists (fixes retraining in notebooks)\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    if AcceleratorState._shared_state != {}:\n",
    "        AcceleratorState._reset_state()\n",
    "        print(\"✓ Accelerator state reset\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reinitialize trainer to ensure clean state\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                   \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,     \n",
    "    args=training_args,\n",
    ")\n",
    "print(\"✓ Trainer reinitialized with clean accelerator state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:30:08.633488Z",
     "iopub.status.busy": "2026-02-18T04:30:08.633150Z",
     "iopub.status.idle": "2026-02-18T04:54:17.050684Z",
     "shell.execute_reply": "2026-02-18T04:54:17.049947Z",
     "shell.execute_reply.started": "2026-02-18T04:30:08.633456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56/3710090654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                                     \u001b[0mgrad_norm_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplicit_replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mgrad_norm_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m                                     _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2716\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2894\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2832\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2833\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_dtype_grads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                     torch._amp_foreach_non_finite_check_and_unscale_(\n\u001b[0m\u001b[1;32m    284\u001b[0m                         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mper_device_found_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING DIAGNOSTICS\n",
      "============================================================\n",
      "⚠️  No training history available\n"
     ]
    }
   ],
   "source": [
    "# Check training history to diagnose issues\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(trainer.state, 'log_history') and len(trainer.state.log_history) > 0:\n",
    "    # Extract loss values\n",
    "    train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "    \n",
    "    print(f\"\\nTraining steps completed: {trainer.state.global_step}\")\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\"  - Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\"  - Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  - Change: {train_losses[-1] - train_losses[0]:.4f}\")\n",
    "    \n",
    "    if eval_losses:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\"  - Best: {min(eval_losses):.4f}\")\n",
    "        print(f\"  - Final: {eval_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Warning if loss didn't decrease\n",
    "    if len(train_losses) > 1 and train_losses[-1] >= train_losses[0]:\n",
    "        print(\"\\n⚠️  WARNING: Training loss did NOT decrease!\")\n",
    "        print(\"   This indicates training failed. Check hyperparameters.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Training loss decreased successfully\")\n",
    "else:\n",
    "    print(\"⚠️  No training history available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:54:17.052075Z",
     "iopub.status.busy": "2026-02-18T04:54:17.051765Z",
     "iopub.status.idle": "2026-02-18T04:54:17.715272Z",
     "shell.execute_reply": "2026-02-18T04:54:17.714580Z",
     "shell.execute_reply.started": "2026-02-18T04:54:17.052042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ../models/final/gemma-2b-financial-lora\n",
      "\n",
      "✓ Model saved successfully!\n",
      "✓ Save location: ../models/final/gemma-2b-financial-lora\n",
      "\n",
      "Saved files (7):\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer.json\n",
      "  - adapter_config.json\n",
      "  - tokenizer_config.json\n",
      "  - tokenizer.model\n",
      "  - README.md\n",
      "  - adapter_model.safetensors\n",
      "\n",
      "The LoRA adapters have been saved locally.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "final_model_path = \"../models/final/gemma-2b-financial-lora\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to: {final_model_path}\")\n",
    "trainer.model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully!\")\n",
    "print(f\"✓ Save location: {final_model_path}\")\n",
    "\n",
    "# List saved files\n",
    "saved_files = os.listdir(final_model_path)\n",
    "print(f\"\\nSaved files ({len(saved_files)}):\")\n",
    "for file in saved_files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file}\")\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")\n",
    "\n",
    "print(\"\\nThe LoRA adapters have been saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Inference & Evaluation\n",
    "\n",
    "Test the fine-tuned model with sample questions and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:54:17.716332Z",
     "iopub.status.busy": "2026-02-18T04:54:17.716133Z",
     "iopub.status.idle": "2026-02-18T04:54:17.722618Z",
     "shell.execute_reply": "2026-02-18T04:54:17.722016Z",
     "shell.execute_reply.started": "2026-02-18T04:54:17.716312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inference function ready!\n"
     ]
    }
   ],
   "source": [
    "# Inference function\n",
    "def generate_response(instruction, input_context, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate response for a given instruction and context.\n",
    "    \n",
    "    Args:\n",
    "        instruction: The question to answer\n",
    "        input_context: The context from 10-K filing\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_context}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    response = full_output.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✓ Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:54:17.723684Z",
     "iopub.status.busy": "2026-02-18T04:54:17.723429Z",
     "iopub.status.idle": "2026-02-18T04:54:57.045752Z",
     "shell.execute_reply": "2026-02-18T04:54:57.045096Z",
     "shell.execute_reply.started": "2026-02-18T04:54:17.723654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFERENCE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 1 - ABBV\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "How will the Inflation Reduction Act of 2022 impact Medicare Part D and Part B drug pricing?\n",
      "\n",
      "Context (excerpt):\n",
      "The Inflation Reduction Act of 2022 requires the government to set prices for select high expenditure Medicare Part D drugs and Part B drugs that are more than nine years (for small-molecule drugs) or...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "The Inflation Reduction Act requires government-set prices for select drugs more than nine years from FDA approval and mandates rebates when drug prices increase faster than inflation.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 2 - ABNB\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "What were the key business metrics tracked by the company in 2023?\n",
      "\n",
      "Context (excerpt):\n",
      "Key Business Metrics We review the following key business metrics to measure our performance, identify trends, formulate financial projections, and make strategic decisions. The following table summar...\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "The key business metrics tracked were Nights and Experiences Booked and Gross Booking Value.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 3 - IRM\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "What was the total growth investment capital expenditures in 2022?\n",
      "\n",
      "Context (excerpt):\n",
      "The total growth investment capital expenditures for 2022 amounted to $819,531....\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "$819,531\n",
      "────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Test with sample examples from test set\n",
    "print(\"=\"*60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_examples = 3\n",
    "for i in range(min(num_examples, len(test_data))):\n",
    "    example = test_data[i]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i+1} - {example['ticker']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nQuestion:\\n{example['instruction']}\")\n",
    "    print(f\"\\nContext (excerpt):\\n{example['input'][:200]}...\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        example['instruction'], \n",
    "        example['input'],\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"MODEL PREDICTION:\\n{prediction}\")\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"GROUND TRUTH:\\n{example['output']}\")\n",
    "    print(f\"{'─'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T04:54:57.046874Z",
     "iopub.status.busy": "2026-02-18T04:54:57.046648Z",
     "iopub.status.idle": "2026-02-18T05:15:39.556546Z",
     "shell.execute_reply": "2026-02-18T05:15:39.555866Z",
     "shell.execute_reply.started": "2026-02-18T04:54:57.046852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING ON TEST SET\n",
      "============================================================\n",
      "\n",
      "Generating predictions for 350 test examples...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b304a08de904692bc2a52f32303841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ROUGE Scores (on 100 test examples):\n",
      "  - ROUGE-1: 0.0061\n",
      "  - ROUGE-2: 0.0000\n",
      "  - ROUGE-L: 0.0060\n",
      "  - ROUGE-Lsum: 0.0059\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_data)} test examples...\\n\")\n",
    "\n",
    "for example in tqdm(test_data[:100], desc=\"Evaluating\"):  # Evaluate on first 100 for speed\n",
    "    pred = generate_response(\n",
    "        example['instruction'], \n",
    "        example['input'],\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    predictions.append(pred)\n",
    "    references.append(example['output'])\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nROUGE Scores (on {len(predictions)} test examples):\")\n",
    "print(f\"  - ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"  - ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"  - ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"  - ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T05:28:25.958433Z",
     "iopub.status.busy": "2026-02-18T05:28:25.958055Z",
     "iopub.status.idle": "2026-02-18T05:28:25.967351Z",
     "shell.execute_reply": "2026-02-18T05:28:25.966313Z",
     "shell.execute_reply.started": "2026-02-18T05:28:25.958395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type set is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56/1978469331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n✓ Evaluation results saved to: {results_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Save evaluation results\n",
    "results = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"method\": \"QLoRA\",\n",
    "    \"lora_config\": {\n",
    "        \"r\": peft_config.r,\n",
    "        \"alpha\": peft_config.lora_alpha,\n",
    "        \"dropout\": peft_config.lora_dropout,\n",
    "        \"target_modules\": peft_config.target_modules\n",
    "    },\n",
    "    \"training_examples\": len(train_data),\n",
    "    \"eval_examples\": len(predictions),\n",
    "    \"rouge_scores\": rouge_scores,\n",
    "    \"sample_predictions\": [\n",
    "        {\n",
    "            \"instruction\": test_data[i]['instruction'],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"reference\": references[i]\n",
    "        }\n",
    "        for i in range(min(5, len(predictions)))\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_path = \"../outputs/results/evaluation_results.json\"\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Evaluation results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Interactive Testing (Optional)\n",
    "\n",
    "Test the model with custom questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T05:28:32.506808Z",
     "iopub.status.busy": "2026-02-18T05:28:32.506079Z",
     "iopub.status.idle": "2026-02-18T05:28:42.347273Z",
     "shell.execute_reply": "2026-02-18T05:28:42.346645Z",
     "shell.execute_reply.started": "2026-02-18T05:28:32.506777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUSTOM QUESTION TEST\n",
      "============================================================\n",
      "\n",
      "Question: What was the company's total revenue?\n",
      "\n",
      "Context: The company reported strong financial performance for fiscal year 2023. \n",
      "Total revenue increased by 15% year-over-year to reach $26.97 billion. \n",
      "This growth was primarily driven by increased demand for our products and services.\n",
      "\n",
      "Model Response:\n",
      "TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify these to test your own questions\n",
    "custom_instruction = \"What was the company's total revenue?\"\n",
    "custom_context = \"\"\"The company reported strong financial performance for fiscal year 2023. \n",
    "Total revenue increased by 15% year-over-year to reach $26.97 billion. \n",
    "This growth was primarily driven by increased demand for our products and services.\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOM QUESTION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuestion: {custom_instruction}\")\n",
    "print(f\"\\nContext: {custom_context}\")\n",
    "\n",
    "response = generate_response(custom_instruction, custom_context, max_new_tokens=150)\n",
    "\n",
    "print(f\"\\nModel Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export Model (Optional)\n",
    "\n",
    "Upload to HuggingFace Hub or download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-18T05:15:39.568896Z",
     "iopub.status.idle": "2026-02-18T05:15:39.569127Z",
     "shell.execute_reply": "2026-02-18T05:15:39.569031Z",
     "shell.execute_reply.started": "2026-02-18T05:15:39.569016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optional: Push to HuggingFace Hub\n",
    "# Uncomment and configure if you want to share your model\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# \n",
    "# model_id = \"your-username/gemma-2b-financial-qa-lora\"\n",
    "# trainer.model.push_to_hub(model_id)\n",
    "# tokenizer.push_to_hub(model_id)\n",
    "# \n",
    "# print(f\"✓ Model pushed to HuggingFace Hub: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T05:28:45.237529Z",
     "iopub.status.busy": "2026-02-18T05:28:45.236867Z",
     "iopub.status.idle": "2026-02-18T05:28:48.059507Z",
     "shell.execute_reply": "2026-02-18T05:28:48.058846Z",
     "shell.execute_reply.started": "2026-02-18T05:28:45.237496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model archived to: ../gemma-2b-financial-qa-model.zip\n",
      "\n",
      "You can download this file and use it locally or upload to HuggingFace.\n"
     ]
    }
   ],
   "source": [
    "# Create a downloadable archive (for local use)\n",
    "import shutil\n",
    "\n",
    "archive_path = \"../models/final/gemma-2b-financial-qa-model\"\n",
    "if os.path.exists(archive_path + \".zip\"):\n",
    "    os.remove(archive_path + \".zip\")\n",
    "\n",
    "shutil.make_archive(archive_path, 'zip', final_model_path)\n",
    "print(f\"✓ Model archived to: {archive_path}.zip\")\n",
    "print(\"\\nThe model archive has been saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting Guide\n",
    "\n",
    "### If You See Repetitive Text (Model Collapse):\n",
    "\n",
    "**Symptoms**: Model outputs \"TheTheThe...\" or \"$$$...\" repeatedly\n",
    "\n",
    "**Common Causes & Fixes**:\n",
    "1. **Gradient clipping disabled** → Set `max_grad_norm=1.0` ✓ (Fixed above)\n",
    "2. **Learning rate too high** → Reduced to `1e-4` from `2e-4` ✓ (Fixed above)\n",
    "3. **Not enough training** → Increased to 3 epochs ✓ (Fixed above)\n",
    "4. **No monitoring** → Enabled `eval_strategy=\"steps\"` ✓ (Fixed above)\n",
    "\n",
    "### What to Check After Retraining:\n",
    "- Training loss should **decrease** (check cell 27 output)\n",
    "- Validation loss should **decrease** (monitored every 500 steps)\n",
    "- First inference examples should show **coherent text**, not repetition\n",
    "\n",
    "### If Problems Persist:\n",
    "- Increase `num_train_epochs` to 4-5\n",
    "- Try `learning_rate=5e-5` (even lower)\n",
    "- Increase `max_seq_length` to 512 (more context)\n",
    "- Check that your GPU has enough VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "1. ✓ Preprocessed 5,000 financial Q&A examples into Alpaca format\n",
    "2. ✓ Fine-tuned Gemma-2B using QLoRA (4-bit quantization)\n",
    "3. ✓ Evaluated the model on test data using ROUGE metrics\n",
    "4. ✓ Saved the fine-tuned LoRA adapters\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different hyperparameters (learning rate, LoRA rank, etc.)\n",
    "- Try fine-tuning on the full dataset (7,000+ examples)\n",
    "- Test with real-world financial questions\n",
    "- Integrate into a RAG (Retrieval-Augmented Generation) pipeline\n",
    "- Deploy as an API using FastAPI or Gradio\n",
    "\n",
    "**Model Usage:**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and LoRA adapters\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/lora/adapters\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/lora/adapters\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9504791,
     "sourceId": 14858734,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
