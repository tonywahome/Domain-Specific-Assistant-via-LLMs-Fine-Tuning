{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de90957",
   "metadata": {},
   "source": [
    "# Domain-Specific Financial Assistant via LLM Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes **Gemma-2B** using **QLoRA** (4-bit quantization) on the Financial-QA-10k dataset to create a domain-specific assistant for financial question answering.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Google Gemma-2B\n",
    "- **Method**: QLoRA (4-bit quantization with LoRA adapters)\n",
    "- **Dataset**: Financial-QA-10k (3,000 samples from SEC 10-K filings)\n",
    "- **Format**: Alpaca instruction-response template\n",
    "- **Hardware**: Optimized for Kaggle T4/P100 GPUs\n",
    "\n",
    "## Sections\n",
    "1. Environment Setup\n",
    "2. Data Preprocessing\n",
    "3. Model Configuration\n",
    "4. Training\n",
    "5. Inference & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffd30a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Install required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db57a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b40fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.40.0 \\\n",
    "    peft>=0.10.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    accelerate>=0.27.0 \\\n",
    "    bitsandbytes>=0.49.2 \\\n",
    "    trl>=0.8.0 \\\n",
    "    sentencepiece>=0.2.0 \\\n",
    "    evaluate>=0.4.1 \\\n",
    "    rouge-score>=0.1.2 \\\n",
    "    scikit-learn>=1.4.0\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7cb304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 18:28:21.866188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771525701.887777     154 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771525701.894201     154 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771525701.912246     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771525701.912263     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771525701.912265     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771525701.912268     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3058972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Hugging Face (for model uploads)\n",
    "# Set HUGGINGFACE_TOKEN environment variable or use notebook secrets\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "if 'HUGGINGFACE_TOKEN' in os.environ:\n",
    "    login(token=os.environ['HUGGINGFACE_TOKEN'])\n",
    "    print(\"✓ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"⚠ HUGGINGFACE_TOKEN not set - skipping HF login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40258985",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "Load and preprocess the Financial-QA-10k dataset into Alpaca format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c6c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Auto-detect dataset path (Kaggle or local)\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    # Kaggle environment - adjust this path based on your Kaggle dataset name\n",
    "    RAW_DATA_PATH = \"/kaggle/input/financial-qa-10k/Financial-QA-10k.csv\"\n",
    "else:\n",
    "    # Local environment\n",
    "    RAW_DATA_PATH = \"../dataset/Financial-QA-10k.csv\"\n",
    "\n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "MAX_SAMPLES = 3000\n",
    "MAX_SEQ_LENGTH = 512\n",
    "TRAIN_RATIO = 0.90\n",
    "VAL_RATIO = 0.05\n",
    "TEST_RATIO = 0.05\n",
    "\n",
    "# Alpaca prompt template\n",
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "print(f\"Dataset path: {RAW_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4773ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle environment - outputs will be saved to /kaggle/working/\n",
      "\n",
      "Output directories:\n",
      "  Models: /kaggle/working/models/final/gemma-2b-financial-lora\n",
      "  Outputs: /kaggle/working/outputs\n",
      "  Results: /kaggle/working/outputs/results\n",
      "\n",
      "✓ All output directories created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup output paths - Auto-detect for Kaggle or local environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    # Running on Kaggle - save to /kaggle/working/\n",
    "    BASE_OUTPUT_DIR = \"/kaggle/working\"\n",
    "    MODELS_DIR = f\"{BASE_OUTPUT_DIR}/models/final/gemma-2b-financial-lora\"\n",
    "    OUTPUTS_DIR = f\"{BASE_OUTPUT_DIR}/outputs\"\n",
    "    RESULTS_DIR = f\"{BASE_OUTPUT_DIR}/outputs/results\"\n",
    "    ARCHIVE_DIR = f\"{BASE_OUTPUT_DIR}/models/final\"\n",
    "    print(\"Kaggle environment - outputs will be saved to /kaggle/working/\")\n",
    "else:\n",
    "    # Running locally - save to industry folder\n",
    "    # Get the notebook's directory and go up one level to industry/\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    INDUSTRY_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "    \n",
    "    BASE_OUTPUT_DIR = str(INDUSTRY_DIR)\n",
    "    MODELS_DIR = str(INDUSTRY_DIR / \"models\" / \"final\" / \"gemma-2b-financial-lora\")\n",
    "    OUTPUTS_DIR = str(INDUSTRY_DIR / \"outputs\")\n",
    "    RESULTS_DIR = str(INDUSTRY_DIR / \"outputs\" / \"results\")\n",
    "    ARCHIVE_DIR = str(INDUSTRY_DIR / \"models\" / \"final\")\n",
    "    print(f\"Local environment - outputs will be saved to {INDUSTRY_DIR}\")\n",
    "\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  Models: {MODELS_DIR}\")\n",
    "print(f\"  Outputs: {OUTPUTS_DIR}\")\n",
    "print(f\"  Results: {RESULTS_DIR}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(ARCHIVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ All output directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e77d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by cleaning whitespace and standardizing formatting.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
    "    text = re.sub(r'\\$\\s+', '$', text)  # Remove space after dollar sign\n",
    "    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def create_alpaca_format(row):\n",
    "    \"\"\"Convert a dataset row into Alpaca format.\"\"\"\n",
    "    return {\n",
    "        \"instruction\": normalize_text(row['question']),\n",
    "        \"input\": normalize_text(row['context']),\n",
    "        \"output\": normalize_text(row['answer']),\n",
    "        \"ticker\": row['ticker'],\n",
    "        \"filing\": row['filing']\n",
    "    }\n",
    "\n",
    "\n",
    "def truncate_context(text, tokenizer, max_tokens=1500):\n",
    "    \"\"\"Truncate context to fit within token limit.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    \n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Try to end at sentence boundary\n",
    "    sentences = truncated_text.split('. ')\n",
    "    if len(sentences) > 1:\n",
    "        truncated_text = '. '.join(sentences[:-1]) + '.'\n",
    "    \n",
    "    return truncated_text\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f907acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "✓ Loaded 7000 examples\n",
      "\n",
      "Companies: ['NVDA', 'AAPL', 'TSLA', 'LULU', 'PG', 'COST', 'ABNB', 'MSFT', 'BRK-A', 'META', 'AXP', 'PTON', 'SBUX', 'NKE', 'PLTR', 'AMZN', 'NFLX', 'GOOGL', 'ABBV', 'V', 'GME', 'AMC', 'CRM', 'LLY', 'AVGO', 'UNH', 'JNJ', 'HD', 'WMT', 'AMD', 'CVX', 'BAC', 'KO', 'T', 'AZO', 'CAT', 'SCHW', 'CMG', 'CB', 'CMCSA', 'CVS', 'DVA', 'DAL', 'DLTR', 'EBAY', 'EA', 'ENPH', 'EFX', 'ETSY', 'FDX', 'F', 'GRMN', 'GIS', 'GM', 'GILD', 'GS', 'HAS', 'HSY', 'HPE', 'HLT', 'HPQ', 'HUM', 'IBM', 'ICE', 'INTU', 'IRM', 'JPM', 'KR', 'LVS']\n",
      "\n",
      "Company distribution:\n",
      "ticker\n",
      "JNJ     200\n",
      "AAPL    100\n",
      "TSLA    100\n",
      "LULU    100\n",
      "NVDA    100\n",
      "       ... \n",
      "INTU    100\n",
      "IRM     100\n",
      "JPM     100\n",
      "KR      100\n",
      "LVS     100\n",
      "Name: count, Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "print(f\"✓ Loaded {len(df)} examples\")\n",
    "print(f\"\\nCompanies: {df['ticker'].unique().tolist()}\")\n",
    "print(f\"\\nCompany distribution:\\n{df['ticker'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1557a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling 3000 examples (stratified by ticker)...\n",
      "✓ Selected 3000 examples\n",
      "\n",
      "Sampled distribution:\n",
      "ticker\n",
      "JNJ      86\n",
      "CMCSA    43\n",
      "TSLA     43\n",
      "ABBV     43\n",
      "UNH      43\n",
      "         ..\n",
      "GRMN     42\n",
      "HLT      42\n",
      "DLTR     42\n",
      "GIS      42\n",
      "ETSY     42\n",
      "Name: count, Length: 69, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154/2908356001.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Sample data (stratified by company ticker)\n",
    "print(f\"\\nSampling {MAX_SAMPLES} examples (stratified by ticker)...\")\n",
    "\n",
    "if len(df) > MAX_SAMPLES:\n",
    "    df_sampled = df.groupby('ticker', group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=MAX_SAMPLES/len(df), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Adjust to exact count\n",
    "    if len(df_sampled) < MAX_SAMPLES:\n",
    "        additional = df.drop(df_sampled.index).sample(\n",
    "            n=MAX_SAMPLES - len(df_sampled), random_state=42\n",
    "        )\n",
    "        df_sampled = pd.concat([df_sampled, additional]).reset_index(drop=True)\n",
    "    elif len(df_sampled) > MAX_SAMPLES:\n",
    "        df_sampled = df_sampled.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_sampled = df.copy()\n",
    "\n",
    "print(f\"✓ Selected {len(df_sampled)} examples\")\n",
    "print(f\"\\nSampled distribution:\\n{df_sampled['ticker'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de477cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer: google/gemma-2b...\n",
      "✓ Tokenizer loaded (vocab size: 256000)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bd6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting to Alpaca format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854543fc32ce4f389c58e4018c65c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Formatted 3000 examples\n",
      "✓ Truncated 1 contexts to fit within 512 tokens\n"
     ]
    }
   ],
   "source": [
    "# Convert to Alpaca format and truncate if needed\n",
    "print(\"\\nConverting to Alpaca format...\")\n",
    "formatted_data = []\n",
    "truncated_count = 0\n",
    "\n",
    "for idx, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Processing\"):\n",
    "    example = create_alpaca_format(row)\n",
    "    \n",
    "    # Check sequence length\n",
    "    full_prompt = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    token_count = len(tokenizer.encode(full_prompt, add_special_tokens=True))\n",
    "    \n",
    "    # Truncate if needed\n",
    "    if token_count > MAX_SEQ_LENGTH:\n",
    "        overhead = len(tokenizer.encode(\n",
    "            ALPACA_TEMPLATE.format(\n",
    "                instruction=example['instruction'],\n",
    "                input=\"\",\n",
    "                output=example['output']\n",
    "            ),\n",
    "            add_special_tokens=True\n",
    "        ))\n",
    "        \n",
    "        max_context_tokens = MAX_SEQ_LENGTH - overhead - 50\n",
    "        example['input'] = truncate_context(example['input'], tokenizer, max_context_tokens)\n",
    "        truncated_count += 1\n",
    "    \n",
    "    formatted_data.append(example)\n",
    "\n",
    "print(f\"\\n✓ Formatted {len(formatted_data)} examples\")\n",
    "print(f\"✓ Truncated {truncated_count} contexts to fit within {MAX_SEQ_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dca27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing sequence lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f141bd6e01d40efbe9cc49214d6db7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Measuring:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Token length statistics:\n",
      "   Min: 41\n",
      "   Max: 466\n",
      "   Mean: 136.1\n",
      "   Median: 128.0\n",
      "   95th percentile: 214.0\n",
      "   99th percentile: 283.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze sequence lengths\n",
    "print(\"\\nAnalyzing sequence lengths...\")\n",
    "lengths = []\n",
    "\n",
    "for example in tqdm(formatted_data, desc=\"Measuring\"):\n",
    "    prompt = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    lengths.append(len(tokens))\n",
    "\n",
    "print(f\"\\n✓ Token length statistics:\")\n",
    "print(f\"   Min: {min(lengths)}\")\n",
    "print(f\"   Max: {max(lengths)}\")\n",
    "print(f\"   Mean: {np.mean(lengths):.1f}\")\n",
    "print(f\"   Median: {np.median(lengths):.1f}\")\n",
    "print(f\"   95th percentile: {np.percentile(lengths, 95):.1f}\")\n",
    "print(f\"   99th percentile: {np.percentile(lengths, 99):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e797c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data (train: 90%, val: 5%, test: 5%)...\n",
      "✓ Train: 2700 examples\n",
      "✓ Validation: 150 examples\n",
      "✓ Test: 150 examples\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test\n",
    "print(f\"\\nSplitting data (train: {TRAIN_RATIO:.0%}, val: {VAL_RATIO:.0%}, test: {TEST_RATIO:.0%})...\")\n",
    "\n",
    "# First split: train + (val + test)\n",
    "train_data, temp_data = train_test_split(\n",
    "    formatted_data,\n",
    "    train_size=TRAIN_RATIO,\n",
    "    random_state=42,\n",
    "    stratify=[d['ticker'] for d in formatted_data]\n",
    ")\n",
    "\n",
    "# Second split: val and test\n",
    "val_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    train_size=val_ratio_adjusted,\n",
    "    random_state=42,\n",
    "    stratify=[d['ticker'] for d in temp_data]\n",
    ")\n",
    "\n",
    "print(f\"✓ Train: {len(train_data)} examples\")\n",
    "print(f\"✓ Validation: {len(val_data)} examples\")\n",
    "print(f\"✓ Test: {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "966bbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE TRAINING EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Ticker: SBUX\n",
      "\n",
      "Instruction:\n",
      "How did the International segment's revenue performance in fiscal 2023 compare to fiscal 2022, despite COVID-19 impacts?\n",
      "\n",
      "Input (context):\n",
      "For the International segment, despite COVID-19 pandemic-related headwinds in China in the first half of the year, revenue grew 8% in fiscal 2023 compared to fiscal 2022....\n",
      "\n",
      "Output:\n",
      "The International segment's revenue grew by 8% in fiscal 2023 compared to fiscal 2022, despite COVID-19 related challenges in China during the first half of the year.\n"
     ]
    }
   ],
   "source": [
    "# Display sample examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "sample = train_data[0]\n",
    "print(f\"\\nTicker: {sample['ticker']}\")\n",
    "print(f\"\\nInstruction:\\n{sample['instruction']}\")\n",
    "print(f\"\\nInput (context):\\n{sample['input'][:300]}...\")  # Show first 300 chars\n",
    "print(f\"\\nOutput:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698bc38",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Configuration\n",
    "\n",
    "Set up QLoRA configuration and load the base model with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769e4f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_267/3888438812.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# QLoRA Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bnb_config = BitsAndBytesConfig(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbnb_4bit_quant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nf4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbnb_4bit_compute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# QLoRA Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"QLoRA (4-bit quantization) configuration:\")\n",
    "print(f\"  - Quantization type: NF4 (4-bit)\")\n",
    "print(f\"  - Compute dtype: bfloat16\")\n",
    "print(f\"  - Double quantization: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ed1015",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LoraConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_267/402816206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LoRA Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m peft_config = LoraConfig(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# LoRA rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlora_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# LoRA scaling factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlora_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Dropout probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LoraConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,  # LoRA scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layer\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"\\nLoRA configuration:\")\n",
    "print(f\"  - Rank (r): {peft_config.r}\")\n",
    "print(f\"  - Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  - Dropout: {peft_config.lora_dropout}\")\n",
    "print(f\"  - Target modules: {peft_config.target_modules}\")\n",
    "print(f\"  - Task type: {peft_config.task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4386466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: google/gemma-2b...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b68b82417f4799b6e0b9f34584dcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a226aa4ea947528fd26274d53a042a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7402e604fb40d6a0744b806394514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34a99afd8ec4721b3f6fbaef4f9ea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af15c4b1b3e7438481a6122349d6d192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9812e4da39144e193878b4b95aff8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded and LoRA adapters applied!\n",
      "\n",
      "Trainable params: 921,600 || All params: 1,516,189,696 || Trainable%: 0.06%\n"
     ]
    }
   ],
   "source": [
    "# Load base model with quantization\n",
    "print(f\"\\nLoading model: {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},           # Pin to first GPU to avoid DataParallel\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.enable_input_require_grads()\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"✓ Model loaded and LoRA adapters applied!\")\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"\\nTrainable params: {trainable_params:,} || \"\n",
    "          f\"All params: {all_param:,} || \"\n",
    "          f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96408045",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training\n",
    "\n",
    "Prepare datasets and train the model using the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037f6853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb461f3e9dc84f04bbd4e0c99efd8845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef212d0c9455402099a633847f98e4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e44ed1113e24203a53aad5f7383ec2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared training dataset: 2700 examples\n",
      "✓ Prepared validation dataset: 150 examples\n",
      "✓ Prepared test dataset: 150 examples\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format example into Alpaca template for training.\"\"\"\n",
    "    text = ALPACA_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        input=example['input'],\n",
    "        output=example['output']\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in train_data])\n",
    "val_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in val_data])\n",
    "test_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in test_data])\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_instruction)\n",
    "val_dataset = val_dataset.map(format_instruction)\n",
    "test_dataset = test_dataset.map(format_instruction)\n",
    "\n",
    "print(f\"✓ Prepared training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"✓ Prepared validation dataset: {len(val_dataset)} examples\")\n",
    "print(f\"✓ Prepared test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f218216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  - Epochs: 3\n",
      "  - Batch size (per device): 2\n",
      "  - Gradient accumulation steps: 8\n",
      "  - Effective batch size: 16\n",
      "  - Learning rate: 0.0001\n",
      "  - LR scheduler: SchedulerType.COSINE\n",
      "  - Warmup steps: 100\n",
      "  - Max grad norm: 1.0\n",
      "  - Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n",
      "  - FP16: False\n",
      "  - Evaluation strategy: IntervalStrategy.STEPS\n",
      "  - Gradient checkpointing: False\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUTS_DIR,\n",
    "    num_train_epochs=3,  # Increased from 2 to 3\n",
    "    per_device_train_batch_size=2,  # Increased from 1 to 2 if GPU allows\n",
    "    gradient_accumulation_steps=8,  # Adjusted to keep effective batch = 16\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,  # Reduced from 2e-4 - lower learning rate for stability\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",  # Changed to \"steps\" to monitor training\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,  # Keep 2 checkpoints to compare\n",
    "    fp16=False,  # Enable fp16 for better training stability\n",
    "    bf16=False,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=1.0,  # CRITICAL FIX: Enable gradient clipping\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # Load best checkpoint at end\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"  - Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  - Max grad norm: {training_args.max_grad_norm}\")\n",
    "print(f\"  - Optimizer: {training_args.optim}\")\n",
    "print(f\"  - FP16: {training_args.fp16}\")\n",
    "print(f\"  - Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  - Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ed9e626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TrainingArguments Precision Settings ---\n",
      "fp16 (Half Precision): False\n",
      "bf16 (Bfloat16): False\n",
      "\n",
      "--- Model Hardware Precision ---\n",
      "Model primary dtype: torch.float32\n",
      "Quantization compute_dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check TrainingArguments configuration\n",
    "print(\"--- TrainingArguments Precision Settings ---\")\n",
    "print(f\"fp16 (Half Precision): {training_args.fp16}\")\n",
    "print(f\"bf16 (Bfloat16): {training_args.bf16}\")\n",
    "\n",
    "# Check the actual model data type\n",
    "print(\"\\n--- Model Hardware Precision ---\")\n",
    "print(f\"Model primary dtype: {model.dtype}\")\n",
    "\n",
    "# If using QLoRA, check the 4-bit compute dtype\n",
    "if hasattr(model, \"config\") and hasattr(model.config, \"quantization_config\"):\n",
    "    compute_dtype = model.config.quantization_config.bnb_4bit_compute_dtype\n",
    "    print(f\"Quantization compute_dtype: {compute_dtype}\")\n",
    "\n",
    "# Verify CUDA compatibility for the selected precision\n",
    "if training_args.bf16:\n",
    "    cuda_supports_bf16 = torch.cuda.is_bf16_supported()\n",
    "    print(f\"\\n--- Hardware Compatibility ---\")\n",
    "    print(f\"GPU supports bf16: {cuda_supports_bf16}\")\n",
    "    if not cuda_supports_bf16:\n",
    "        print(\"WARNING: You are using bf16 on a GPU that does not support it (like Tesla T4). This will cause errors.\")\n",
    "\n",
    "if training_args.fp16:\n",
    "    print(f\"\\n--- Hardware Compatibility ---\")\n",
    "    print(f\"FP16 enabled: Compatible with most GPUs including T4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6480fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136abe113434f348a8c41b1b3600e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c74751689f456386915536f1965064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028bf94662644e6d92f0b890dadbd417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c584a1616c5c4e4d8a19b186d4f57eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138b3d5a9deb4d9791f68a11f153d1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba03fe306d34e4784ccef32333226a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer reinitialized with clean accelerator state\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache and reset accelerator state before training\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset accelerator state if it exists (fixes retraining in notebooks)\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    if AcceleratorState._shared_state != {}:\n",
    "        AcceleratorState._reset_state()\n",
    "        print(\"✓ Accelerator state reset\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reinitialize trainer to ensure clean state\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                   \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,     \n",
    "    args=training_args,\n",
    ")\n",
    "print(\"✓ Trainer reinitialized with clean accelerator state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7435cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 50:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.140100</td>\n",
       "      <td>1.059626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c61142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING DIAGNOSTICS\n",
      "============================================================\n",
      "\n",
      "Training steps completed: 507\n",
      "\n",
      "Training Loss:\n",
      "  - Initial: 2.3173\n",
      "  - Final: 1.1401\n",
      "  - Change: -1.1772\n",
      "\n",
      "Validation Loss:\n",
      "  - Best: 1.0596\n",
      "  - Final: 1.0596\n",
      "\n",
      "✓ Training loss decreased successfully\n"
     ]
    }
   ],
   "source": [
    "# Check training history to diagnose issues\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(trainer.state, 'log_history') and len(trainer.state.log_history) > 0:\n",
    "    # Extract loss values\n",
    "    train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "    \n",
    "    print(f\"\\nTraining steps completed: {trainer.state.global_step}\")\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\"  - Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\"  - Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  - Change: {train_losses[-1] - train_losses[0]:.4f}\")\n",
    "    \n",
    "    if eval_losses:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\"  - Best: {min(eval_losses):.4f}\")\n",
    "        print(f\"  - Final: {eval_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Warning if loss didn't decrease\n",
    "    if len(train_losses) > 1 and train_losses[-1] >= train_losses[0]:\n",
    "        print(\"\\n⚠️  WARNING: Training loss did NOT decrease!\")\n",
    "        print(\"   This indicates training failed. Check hyperparameters.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Training loss decreased successfully\")\n",
    "else:\n",
    "    print(\"⚠️  No training history available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00ad4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /kaggle/working/models/final/gemma-2b-financial-lora\n",
      "\n",
      "✓ Model saved successfully!\n",
      "✓ Save location: /kaggle/working/models/final/gemma-2b-financial-lora\n",
      "\n",
      "Saved files (7):\n",
      "  - tokenizer_config.json\n",
      "  - tokenizer.json\n",
      "  - adapter_config.json\n",
      "  - special_tokens_map.json\n",
      "  - README.md\n",
      "  - tokenizer.model\n",
      "  - adapter_model.safetensors\n",
      "\n",
      "The LoRA adapters have been saved locally.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "print(f\"Saving model to: {MODELS_DIR}\")\n",
    "trainer.model.save_pretrained(MODELS_DIR)\n",
    "tokenizer.save_pretrained(MODELS_DIR)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully!\")\n",
    "print(f\"✓ Save location: {MODELS_DIR}\")\n",
    "\n",
    "# List saved files\n",
    "saved_files = os.listdir(MODELS_DIR)\n",
    "print(f\"\\nSaved files ({len(saved_files)}):\")\n",
    "for file in saved_files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")\n",
    "\n",
    "print(\"\\nThe LoRA adapters have been saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e871911",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Inference & Evaluation\n",
    "\n",
    "Test the fine-tuned model with sample questions and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26deb09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inference function ready!\n"
     ]
    }
   ],
   "source": [
    "# Inference function\n",
    "def generate_response(instruction, input_context, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate response for a given instruction and context.\n",
    "    \n",
    "    Args:\n",
    "        instruction: The question to answer\n",
    "        input_context: The context from 10-K filing\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_context}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    response = full_output.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✓ Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5df75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFERENCE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 1 - AZO\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "What criteria did the independent audit use to assess the effectiveness of internal control over financial reporting at the company?\n",
      "\n",
      "Context (excerpt):\n",
      "The independent registered public accounting firm conducted an audit based on the Internal Control-Integrated Framework issued by the Committee of Sponsoring Organizations of the Treadway Commission (...\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "The independent registered public accounting firm conducted an audit based on the 2013 framework issued by the Committee of Sponsoring Organizations of the Treadway Commission.\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "The independent audit assessed the effectiveness of internal control over financial reporting based on criteria established in the Internal Control-Integrated Framework issued by the Committee of Sponsoring Organizations of the Treadway Commission (2013 framework). This framework guides the audit in evaluating whether the company has maintained effective controls over its financial reporting processes.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 2 - GRMN\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "What are Garmin's reportable business segments as identified in their 2023 financial analysis?\n",
      "\n",
      "Context (excerpt):\n",
      "Garmin is organized in the five operating segments of fitness, outdoor, aviation, marine, and auto OEM. These operating segments represent our reportable segments....\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "Garmin's reportable business segments are fitness, outdoor, aviation, marine, and auto OEM.\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "Fitness, outdoor, aviation, marine, and auto OEM\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 3 - DVA\n",
      "============================================================\n",
      "\n",
      "Question:\n",
      "What percentage of U.S. dialysis patient service revenues in 2023 came from Medicare and Medicare Advantage plans?\n",
      "\n",
      "Context (excerpt):\n",
      "For the year ended December 31, 2023, Medicare and Medicare Advantage plans accounted for 56% of U.S. dialysis patient service revenues....\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL PREDICTION:\n",
      "56%\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GROUND TRUTH:\n",
      "56%\n",
      "────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Test with sample examples from test set\n",
    "print(\"=\"*60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_examples = 3\n",
    "for i in range(min(num_examples, len(test_data))):\n",
    "    example = test_data[i]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i+1} - {example['ticker']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nQuestion:\\n{example['instruction']}\")\n",
    "    print(f\"\\nContext (excerpt):\\n{example['input'][:200]}...\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        example['instruction'], \n",
    "        example['input'],\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"MODEL PREDICTION:\\n{prediction}\")\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"GROUND TRUTH:\\n{example['output']}\")\n",
    "    print(f\"{'─'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c10d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING ON TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10a6f92572c4b0eaa625ffb5ac282ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for 150 test examples...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f7d9b36a7c40739910ac2c96fba064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ROUGE Scores (on 100 test examples):\n",
      "  - ROUGE-1: 0.5957\n",
      "  - ROUGE-2: 0.4339\n",
      "  - ROUGE-L: 0.5472\n",
      "  - ROUGE-Lsum: 0.5475\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_data)} test examples...\\n\")\n",
    "\n",
    "for example in tqdm(test_data[:100], desc=\"Evaluating\"):  # Evaluate on first 100 for speed\n",
    "    pred = generate_response(\n",
    "        example['instruction'], \n",
    "        example['input'],\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    predictions.append(pred)\n",
    "    references.append(example['output'])\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nROUGE Scores (on {len(predictions)} test examples):\")\n",
    "print(f\"  - ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"  - ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"  - ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"  - ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbafc588",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_267/2511031022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"QLoRA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"lora_config\": {\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peft_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Save evaluation results\n",
    "results = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"method\": \"QLoRA\",\n",
    "    \"lora_config\": {\n",
    "        \"r\": peft_config.r,\n",
    "        \"alpha\": peft_config.lora_alpha,\n",
    "        \"dropout\": peft_config.lora_dropout,\n",
    "        \"target_modules\": list(peft_config.target_modules) if isinstance(peft_config.target_modules, set) else peft_config.target_modules\n",
    "    },\n",
    "    \"training_examples\": len(train_data),\n",
    "    \"eval_examples\": len(predictions),\n",
    "    \"rouge_scores\": rouge_scores,\n",
    "    \"sample_predictions\": [\n",
    "        {\n",
    "            \"instruction\": test_data[i]['instruction'],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"reference\": references[i]\n",
    "        }\n",
    "        for i in range(min(5, len(predictions)))\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_path = os.path.join(RESULTS_DIR, \"evaluation_results.json\")\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Evaluation results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f9ddb",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Interactive Testing (Optional)\n",
    "\n",
    "Test the model with custom questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39453b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing - modify these to test your own questions\n",
    "custom_instruction = \"What was the company's total revenue?\"\n",
    "custom_context = \"\"\"The company reported strong financial performance for fiscal year 2023. \n",
    "Total revenue increased by 15% year-over-year to reach $26.97 billion. \n",
    "This growth was primarily driven by increased demand for our products and services.\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOM QUESTION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuestion: {custom_instruction}\")\n",
    "print(f\"\\nContext: {custom_context}\")\n",
    "\n",
    "response = generate_response(custom_instruction, custom_context, max_new_tokens=150)\n",
    "\n",
    "print(f\"\\nModel Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ce3b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export Model (Optional)\n",
    "\n",
    "Upload to HuggingFace Hub or download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735db972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to HuggingFace Hub\n",
    "# Uncomment and configure if you want to share your model\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "# \n",
    "model_id = \"your-username/gemma-2b-financial-qa-lora\"\n",
    "trainer.model.push_to_hub(model_id)\n",
    "tokenizer.push_to_hub(model_id)\n",
    "# \n",
    "print(f\"✓ Model pushed to HuggingFace Hub: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a downloadable archive (for local use)\n",
    "import shutil\n",
    "\n",
    "archive_path = os.path.join(ARCHIVE_DIR, \"gemma-2b-financial-qa-model\")\n",
    "if os.path.exists(archive_path + \".zip\"):\n",
    "    os.remove(archive_path + \".zip\")\n",
    "\n",
    "shutil.make_archive(archive_path, 'zip', MODELS_DIR)\n",
    "print(f\"✓ Model archived to: {archive_path}.zip\")\n",
    "print(\"\\nThe model archive has been saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570eb58",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting Guide\n",
    "\n",
    "### If You See Repetitive Text (Model Collapse):\n",
    "\n",
    "**Symptoms**: Model outputs \"TheTheThe...\" or \"$$$...\" repeatedly\n",
    "\n",
    "**Common Causes & Fixes**:\n",
    "1. **Gradient clipping disabled** → Set `max_grad_norm=1.0` ✓ (Fixed above)\n",
    "2. **Learning rate too high** → Reduced to `1e-4` from `2e-4` ✓ (Fixed above)\n",
    "3. **Not enough training** → Increased to 3 epochs ✓ (Fixed above)\n",
    "4. **No monitoring** → Enabled `eval_strategy=\"steps\"` ✓ (Fixed above)\n",
    "\n",
    "### What to Check After Retraining:\n",
    "- Training loss should **decrease** (check cell 27 output)\n",
    "- Validation loss should **decrease** (monitored every 500 steps)\n",
    "- First inference examples should show **coherent text**, not repetition\n",
    "\n",
    "### If Problems Persist:\n",
    "- Increase `num_train_epochs` to 4-5\n",
    "- Try `learning_rate=5e-5` (even lower)\n",
    "- Increase `max_seq_length` to 512 (more context)\n",
    "- Check that your GPU has enough VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b57148",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "1. ✓ Preprocessed 5,000 financial Q&A examples into Alpaca format\n",
    "2. ✓ Fine-tuned Gemma-2B using QLoRA (4-bit quantization)\n",
    "3. ✓ Evaluated the model on test data using ROUGE metrics\n",
    "4. ✓ Saved the fine-tuned LoRA adapters\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different hyperparameters (learning rate, LoRA rank, etc.)\n",
    "- Try fine-tuning on the full dataset (7,000+ examples)\n",
    "- Test with real-world financial questions\n",
    "- Integrate into a RAG (Retrieval-Augmented Generation) pipeline\n",
    "- Deploy as an API using FastAPI or Gradio\n",
    "\n",
    "**Model Usage:**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and LoRA adapters\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/lora/adapters\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/lora/adapters\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
