{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14858734,"sourceType":"datasetVersion","datasetId":9504791}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c94678de","cell_type":"markdown","source":"# Domain-Specific Financial Assistant via LLM Fine-Tuning\n\nThis notebook fine-tunes **Gemma-2B** using **QLoRA** (4-bit quantization) on the Financial-QA-10k dataset to create a domain-specific assistant for financial question answering.\n\n## Overview\n- **Model**: Google Gemma-2B\n- **Method**: QLoRA (4-bit quantization with LoRA adapters)\n- **Dataset**: Financial-QA-10k (5,000 samples from SEC 10-K filings)\n- **Format**: Alpaca instruction-response template\n- **Hardware**: Optimized for Kaggle T4/P100 GPUs\n\n## Sections\n1. Environment Setup\n2. Data Preprocessing\n3. Model Configuration\n4. Training\n5. Inference & Evaluation","metadata":{}},{"id":"05073c6d","cell_type":"markdown","source":"---\n## 1. Environment Setup\n\nInstall required dependencies and set up the environment.","metadata":{}},{"id":"33a8ce2a","cell_type":"code","source":"# Install required packages\n!pip install -q transformers>=4.40.0 \\\n    peft>=0.10.0 \\\n    datasets>=2.18.0 \\\n    accelerate>=0.27.0 \\\n    bitsandbytes>=0.43.0 \\\n    trl>=0.8.0 \\\n    sentencepiece>=0.2.0 \\\n    evaluate>=0.4.1 \\\n    rouge-score>=0.1.2 \\\n    scikit-learn>=1.4.0\n\nprint(\"✓ All packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:00.354724Z","iopub.execute_input":"2026-02-16T14:25:00.355401Z","iopub.status.idle":"2026-02-16T14:25:04.022262Z","shell.execute_reply.started":"2026-02-16T14:25:00.355371Z","shell.execute_reply":"2026-02-16T14:25:04.021453Z"}},"outputs":[{"name":"stdout","text":"✓ All packages installed successfully!\n","output_type":"stream"}],"execution_count":7},{"id":"ff2c0918","cell_type":"code","source":"# Import libraries\nimport os\nimport json\nimport re\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nfrom datasets import Dataset, DatasetDict\nfrom trl import SFTTrainer\nimport evaluate\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.024031Z","iopub.execute_input":"2026-02-16T14:25:04.024721Z","iopub.status.idle":"2026-02-16T14:25:04.033606Z","shell.execute_reply.started":"2026-02-16T14:25:04.024687Z","shell.execute_reply":"2026-02-16T14:25:04.032689Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nVRAM: 15.64 GB\n","output_type":"stream"}],"execution_count":8},{"id":"46c37342","cell_type":"code","source":"\nfrom huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:57:51.693138Z","iopub.execute_input":"2026-02-16T14:57:51.693465Z","iopub.status.idle":"2026-02-16T14:57:51.866871Z","shell.execute_reply.started":"2026-02-16T14:57:51.693433Z","shell.execute_reply":"2026-02-16T14:57:51.866293Z"}},"outputs":[],"execution_count":53},{"id":"6ab54944","cell_type":"markdown","source":"---\n## 2. Data Preprocessing\n\nLoad and preprocess the Financial-QA-10k dataset into Alpaca format.","metadata":{}},{"id":"9cbff4f2","cell_type":"code","source":"# Configuration\nRAW_DATA_PATH = \"../dataset/Financial-QA-10k.csv\"  # Adjust path if needed\nMODEL_NAME = \"google/gemma-2b\"\nMAX_SAMPLES = 5000\nMAX_SEQ_LENGTH = 2048\nTRAIN_RATIO = 0.90\nVAL_RATIO = 0.05\nTEST_RATIO = 0.05\n\n# Alpaca prompt template\nALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}\"\"\"\n\nprint(\"Configuration loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:46.264280Z","iopub.execute_input":"2026-02-16T14:25:46.265021Z","iopub.status.idle":"2026-02-16T14:25:46.269658Z","shell.execute_reply.started":"2026-02-16T14:25:46.264989Z","shell.execute_reply":"2026-02-16T14:25:46.269013Z"}},"outputs":[{"name":"stdout","text":"Configuration loaded successfully!\n","output_type":"stream"}],"execution_count":18},{"id":"fec3cf66","cell_type":"code","source":"# Helper functions\ndef normalize_text(text):\n    \"\"\"Normalize text by cleaning whitespace and standardizing formatting.\"\"\"\n    if pd.isna(text) or text is None:\n        return \"\"\n    \n    text = str(text).strip()\n    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n    text = re.sub(r'\\$\\s+', '$', text)  # Remove space after dollar sign\n    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n    \n    return text\n\n\ndef create_alpaca_format(row):\n    \"\"\"Convert a dataset row into Alpaca format.\"\"\"\n    return {\n        \"instruction\": normalize_text(row['question']),\n        \"input\": normalize_text(row['context']),\n        \"output\": normalize_text(row['answer']),\n        \"ticker\": row['ticker'],\n        \"filing\": row['filing']\n    }\n\n\ndef truncate_context(text, tokenizer, max_tokens=1500):\n    \"\"\"Truncate context to fit within token limit.\"\"\"\n    tokens = tokenizer.encode(text, add_special_tokens=False)\n    \n    if len(tokens) <= max_tokens:\n        return text\n    \n    truncated_tokens = tokens[:max_tokens]\n    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n    \n    # Try to end at sentence boundary\n    sentences = truncated_text.split('. ')\n    if len(sentences) > 1:\n        truncated_text = '. '.join(sentences[:-1]) + '.'\n    \n    return truncated_text\n\nprint(\"Helper functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:26:07.430509Z","iopub.execute_input":"2026-02-16T14:26:07.431078Z","iopub.status.idle":"2026-02-16T14:26:07.438250Z","shell.execute_reply.started":"2026-02-16T14:26:07.431048Z","shell.execute_reply":"2026-02-16T14:26:07.437543Z"}},"outputs":[{"name":"stdout","text":"Helper functions defined!\n","output_type":"stream"}],"execution_count":19},{"id":"60d213e7","cell_type":"code","source":"# Load dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv('/kaggle/input/datasets/antonywambugu/financial-qa-10k/Financial-QA-10k.csv')\nprint(f\"✓ Loaded {len(df)} examples\")\nprint(f\"\\nCompanies: {df['ticker'].unique().tolist()}\")\nprint(f\"\\nCompany distribution:\\n{df['ticker'].value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:27:42.989470Z","iopub.execute_input":"2026-02-16T14:27:42.990064Z","iopub.status.idle":"2026-02-16T14:27:43.111332Z","shell.execute_reply.started":"2026-02-16T14:27:42.990037Z","shell.execute_reply":"2026-02-16T14:27:43.110741Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n✓ Loaded 7000 examples\n\nCompanies: ['NVDA', 'AAPL', 'TSLA', 'LULU', 'PG', 'COST', 'ABNB', 'MSFT', 'BRK-A', 'META', 'AXP', 'PTON', 'SBUX', 'NKE', 'PLTR', 'AMZN', 'NFLX', 'GOOGL', 'ABBV', 'V', 'GME', 'AMC', 'CRM', 'LLY', 'AVGO', 'UNH', 'JNJ', 'HD', 'WMT', 'AMD', 'CVX', 'BAC', 'KO', 'T', 'AZO', 'CAT', 'SCHW', 'CMG', 'CB', 'CMCSA', 'CVS', 'DVA', 'DAL', 'DLTR', 'EBAY', 'EA', 'ENPH', 'EFX', 'ETSY', 'FDX', 'F', 'GRMN', 'GIS', 'GM', 'GILD', 'GS', 'HAS', 'HSY', 'HPE', 'HLT', 'HPQ', 'HUM', 'IBM', 'ICE', 'INTU', 'IRM', 'JPM', 'KR', 'LVS']\n\nCompany distribution:\nticker\nJNJ     200\nAAPL    100\nTSLA    100\nLULU    100\nNVDA    100\n       ... \nINTU    100\nIRM     100\nJPM     100\nKR      100\nLVS     100\nName: count, Length: 69, dtype: int64\n","output_type":"stream"}],"execution_count":23},{"id":"6fde0184","cell_type":"code","source":"# Sample data (stratified by company ticker)\nprint(f\"\\nSampling {MAX_SAMPLES} examples (stratified by ticker)...\")\n\nif len(df) > MAX_SAMPLES:\n    df_sampled = df.groupby('ticker', group_keys=False).apply(\n        lambda x: x.sample(frac=MAX_SAMPLES/len(df), random_state=42)\n    ).reset_index(drop=True)\n    \n    # Adjust to exact count\n    if len(df_sampled) < MAX_SAMPLES:\n        additional = df.drop(df_sampled.index).sample(\n            n=MAX_SAMPLES - len(df_sampled), random_state=42\n        )\n        df_sampled = pd.concat([df_sampled, additional]).reset_index(drop=True)\n    elif len(df_sampled) > MAX_SAMPLES:\n        df_sampled = df_sampled.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\nelse:\n    df_sampled = df.copy()\n\nprint(f\"✓ Selected {len(df_sampled)} examples\")\nprint(f\"\\nSampled distribution:\\n{df_sampled['ticker'].value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:27:51.323952Z","iopub.execute_input":"2026-02-16T14:27:51.324546Z","iopub.status.idle":"2026-02-16T14:27:51.375499Z","shell.execute_reply.started":"2026-02-16T14:27:51.324519Z","shell.execute_reply":"2026-02-16T14:27:51.374811Z"}},"outputs":[{"name":"stdout","text":"\nSampling 5000 examples (stratified by ticker)...\n✓ Selected 5000 examples\n\nSampled distribution:\nticker\nJNJ     144\nICE      75\nGIS      74\nHPE      74\nHUM      73\n       ... \nT        71\nTSLA     71\nUNH      71\nV        71\nWMT      71\nName: count, Length: 69, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2908356001.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sampled = df.groupby('ticker', group_keys=False).apply(\n","output_type":"stream"}],"execution_count":24},{"id":"9d918874","cell_type":"code","source":"# Load tokenizer\nprint(f\"\\nLoading tokenizer: {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Add padding token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:28:07.949764Z","iopub.execute_input":"2026-02-16T14:28:07.950305Z","iopub.status.idle":"2026-02-16T14:28:33.843476Z","shell.execute_reply.started":"2026-02-16T14:28:07.950278Z","shell.execute_reply":"2026-02-16T14:28:33.842820Z"}},"outputs":[{"name":"stdout","text":"\nLoading tokenizer: google/gemma-2b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5f9639ed8044f74a2b57a15bd19baa8"}},"metadata":{}},{"name":"stderr","text":"'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f7c8477a-691b-4ccc-8d23-afeb04e38c5a)')' thrown while requesting HEAD https://huggingface.co/google/gemma-2b/resolve/main/tokenizer.model\nWARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f7c8477a-691b-4ccc-8d23-afeb04e38c5a)')' thrown while requesting HEAD https://huggingface.co/google/gemma-2b/resolve/main/tokenizer.model\nRetrying in 1s [Retry 1/5].\nWARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e88e23447f4d04ae7dda5bda730cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d603a3edb5244d9babb3b7871f68502"}},"metadata":{}},{"name":"stderr","text":"'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5a33515c-e72a-4f63-81fe-1df5273c09b4)')' thrown while requesting HEAD https://huggingface.co/google/gemma-2b/resolve/main/special_tokens_map.json\nWARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5a33515c-e72a-4f63-81fe-1df5273c09b4)')' thrown while requesting HEAD https://huggingface.co/google/gemma-2b/resolve/main/special_tokens_map.json\nRetrying in 1s [Retry 1/5].\nWARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1beb4ad18cd4fcb8ae6170df6a12699"}},"metadata":{}},{"name":"stdout","text":"✓ Tokenizer loaded (vocab size: 256000)\n","output_type":"stream"}],"execution_count":25},{"id":"674123a8","cell_type":"code","source":"# Convert to Alpaca format and truncate if needed\nprint(\"\\nConverting to Alpaca format...\")\nformatted_data = []\ntruncated_count = 0\n\nfor idx, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Processing\"):\n    example = create_alpaca_format(row)\n    \n    # Check sequence length\n    full_prompt = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    token_count = len(tokenizer.encode(full_prompt, add_special_tokens=True))\n    \n    # Truncate if needed\n    if token_count > MAX_SEQ_LENGTH:\n        overhead = len(tokenizer.encode(\n            ALPACA_TEMPLATE.format(\n                instruction=example['instruction'],\n                input=\"\",\n                output=example['output']\n            ),\n            add_special_tokens=True\n        ))\n        \n        max_context_tokens = MAX_SEQ_LENGTH - overhead - 50\n        example['input'] = truncate_context(example['input'], tokenizer, max_context_tokens)\n        truncated_count += 1\n    \n    formatted_data.append(example)\n\nprint(f\"\\n✓ Formatted {len(formatted_data)} examples\")\nprint(f\"✓ Truncated {truncated_count} contexts to fit within {MAX_SEQ_LENGTH} tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:29:27.649218Z","iopub.execute_input":"2026-02-16T14:29:27.649906Z","iopub.status.idle":"2026-02-16T14:29:30.410787Z","shell.execute_reply.started":"2026-02-16T14:29:27.649877Z","shell.execute_reply":"2026-02-16T14:29:30.409921Z"}},"outputs":[{"name":"stdout","text":"\nConverting to Alpaca format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1600f667a9fc4642a04f6a6a43b41054"}},"metadata":{}},{"name":"stdout","text":"\n✓ Formatted 5000 examples\n✓ Truncated 0 contexts to fit within 2048 tokens\n","output_type":"stream"}],"execution_count":26},{"id":"4310d904","cell_type":"code","source":"# Analyze sequence lengths\nprint(\"\\nAnalyzing sequence lengths...\")\nlengths = []\n\nfor example in tqdm(formatted_data, desc=\"Measuring\"):\n    prompt = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n    lengths.append(len(tokens))\n\nprint(f\"\\n✓ Token length statistics:\")\nprint(f\"   Min: {min(lengths)}\")\nprint(f\"   Max: {max(lengths)}\")\nprint(f\"   Mean: {np.mean(lengths):.1f}\")\nprint(f\"   Median: {np.median(lengths):.1f}\")\nprint(f\"   95th percentile: {np.percentile(lengths, 95):.1f}\")\nprint(f\"   99th percentile: {np.percentile(lengths, 99):.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:29:35.680990Z","iopub.execute_input":"2026-02-16T14:29:35.681659Z","iopub.status.idle":"2026-02-16T14:29:37.035513Z","shell.execute_reply.started":"2026-02-16T14:29:35.681612Z","shell.execute_reply":"2026-02-16T14:29:37.034693Z"}},"outputs":[{"name":"stdout","text":"\nAnalyzing sequence lengths...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Measuring:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aec6a79a8db455391283f20ab956521"}},"metadata":{}},{"name":"stdout","text":"\n✓ Token length statistics:\n   Min: 41\n   Max: 559\n   Mean: 136.6\n   Median: 128.0\n   95th percentile: 217.0\n   99th percentile: 284.0\n","output_type":"stream"}],"execution_count":27},{"id":"a376382a","cell_type":"code","source":"# Split data into train/val/test\nprint(f\"\\nSplitting data (train: {TRAIN_RATIO:.0%}, val: {VAL_RATIO:.0%}, test: {TEST_RATIO:.0%})...\")\n\n# First split: train + (val + test)\ntrain_data, temp_data = train_test_split(\n    formatted_data,\n    train_size=TRAIN_RATIO,\n    random_state=42,\n    stratify=[d['ticker'] for d in formatted_data]\n)\n\n# Second split: val and test\nval_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\nval_data, test_data = train_test_split(\n    temp_data,\n    train_size=val_ratio_adjusted,\n    random_state=42,\n    stratify=[d['ticker'] for d in temp_data]\n)\n\nprint(f\"✓ Train: {len(train_data)} examples\")\nprint(f\"✓ Validation: {len(val_data)} examples\")\nprint(f\"✓ Test: {len(test_data)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:29:45.664627Z","iopub.execute_input":"2026-02-16T14:29:45.665176Z","iopub.status.idle":"2026-02-16T14:29:45.680968Z","shell.execute_reply.started":"2026-02-16T14:29:45.665144Z","shell.execute_reply":"2026-02-16T14:29:45.680152Z"}},"outputs":[{"name":"stdout","text":"\nSplitting data (train: 90%, val: 5%, test: 5%)...\n✓ Train: 4500 examples\n✓ Validation: 250 examples\n✓ Test: 250 examples\n","output_type":"stream"}],"execution_count":29},{"id":"4a4402d7","cell_type":"code","source":"# Display sample examples\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE TRAINING EXAMPLE\")\nprint(\"=\"*60)\nsample = train_data[0]\nprint(f\"\\nTicker: {sample['ticker']}\")\nprint(f\"\\nInstruction:\\n{sample['instruction']}\")\nprint(f\"\\nInput (context):\\n{sample['input'][:300]}...\")  # Show first 300 chars\nprint(f\"\\nOutput:\\n{sample['output']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:29:51.709411Z","iopub.execute_input":"2026-02-16T14:29:51.709834Z","iopub.status.idle":"2026-02-16T14:29:51.714734Z","shell.execute_reply.started":"2026-02-16T14:29:51.709807Z","shell.execute_reply":"2026-02-16T14:29:51.714033Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSAMPLE TRAINING EXAMPLE\n============================================================\n\nTicker: GS\n\nInstruction:\nHow do regulatory differences impact the competitive landscape for U.S.-based financial firms like Goldman Sachs when compared to their non-U.S. counterparts?\n\nInput (context):\nThe Dodd-Frank Act places restrictions on U.S.-based financial firms, such as Goldman Sachs, limiting proprietary trading and certain investments. Meanwhile, non-U.S.-based firms may operate with fewer restrictions, creating a competitive disadvantage for U.S.-based firms in the global market....\n\nOutput:\nRegulatory differences can impact the competitive landscape by providing non-U.S.-based firms with more flexibility in certain activities such as proprietary trading and investing in hedge and private equity funds outside the U.S., which are restricted for U.S.-based firms like Goldman Sachs under the Dodd-Frank Act.\n","output_type":"stream"}],"execution_count":30},{"id":"ca316733","cell_type":"markdown","source":"---\n## 3. Model Configuration\n\nSet up QLoRA configuration and load the base model with 4-bit quantization.","metadata":{}},{"id":"57fc340a","cell_type":"code","source":"# QLoRA Configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"QLoRA (4-bit quantization) configuration:\")\nprint(f\"  - Quantization type: NF4 (4-bit)\")\nprint(f\"  - Compute dtype: bfloat16\")\nprint(f\"  - Double quantization: Enabled\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:30:48.686147Z","iopub.execute_input":"2026-02-16T14:30:48.686494Z","iopub.status.idle":"2026-02-16T14:30:48.693585Z","shell.execute_reply.started":"2026-02-16T14:30:48.686462Z","shell.execute_reply":"2026-02-16T14:30:48.692720Z"}},"outputs":[{"name":"stdout","text":"QLoRA (4-bit quantization) configuration:\n  - Quantization type: NF4 (4-bit)\n  - Compute dtype: bfloat16\n  - Double quantization: Enabled\n","output_type":"stream"}],"execution_count":31},{"id":"5f885a4f","cell_type":"code","source":"# LoRA Configuration\npeft_config = LoraConfig(\n    r=16,  # LoRA rank\n    lora_alpha=32,  # LoRA scaling factor\n    lora_dropout=0.05,  # Dropout probability\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nprint(\"\\nLoRA configuration:\")\nprint(f\"  - Rank (r): {peft_config.r}\")\nprint(f\"  - Alpha: {peft_config.lora_alpha}\")\nprint(f\"  - Dropout: {peft_config.lora_dropout}\")\nprint(f\"  - Target modules: {peft_config.target_modules}\")\nprint(f\"  - Task type: {peft_config.task_type}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:30:53.009987Z","iopub.execute_input":"2026-02-16T14:30:53.010819Z","iopub.status.idle":"2026-02-16T14:30:53.016419Z","shell.execute_reply.started":"2026-02-16T14:30:53.010786Z","shell.execute_reply":"2026-02-16T14:30:53.015743Z"}},"outputs":[{"name":"stdout","text":"\nLoRA configuration:\n  - Rank (r): 16\n  - Alpha: 32\n  - Dropout: 0.05\n  - Target modules: {'v_proj', 'k_proj', 'q_proj', 'o_proj'}\n  - Task type: CAUSAL_LM\n","output_type":"stream"}],"execution_count":32},{"id":"7c01b443","cell_type":"code","source":"# Load base model with quantization\nprint(f\"\\nLoading model: {MODEL_NAME}...\")\nprint(\"This may take a few minutes...\\n\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Apply LoRA\nmodel = get_peft_model(model, peft_config)\n\nprint(\"✓ Model loaded and LoRA adapters applied!\")\n\n# Print trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"\\nTrainable params: {trainable_params:,} || \"\n          f\"All params: {all_param:,} || \"\n          f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n\nprint_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:30:56.103883Z","iopub.execute_input":"2026-02-16T14:30:56.104593Z","iopub.status.idle":"2026-02-16T14:31:24.601973Z","shell.execute_reply.started":"2026-02-16T14:30:56.104563Z","shell.execute_reply":"2026-02-16T14:31:24.601146Z"}},"outputs":[{"name":"stdout","text":"\nLoading model: google/gemma-2b...\nThis may take a few minutes...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d24c84954c4a4b8082f18f24bec883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2664b80767c45e3b58d1d435a95267f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e281693439f404593b53f0b37c44d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a1b801083545e091f65549a61d4204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5950ebac7c4ee8a74d24c54b77db8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1227ce34b87e4991baa848dc5d76b246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1d1c022ca7434196f29da8e26c4e0e"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded and LoRA adapters applied!\n\nTrainable params: 3,686,400 || All params: 1,518,954,496 || Trainable%: 0.24%\n","output_type":"stream"}],"execution_count":33},{"id":"b26cc2f0","cell_type":"markdown","source":"---\n## 4. Training\n\nPrepare datasets and train the model using the SFTTrainer.","metadata":{}},{"id":"28feeb7b","cell_type":"code","source":"# Prepare datasets\ndef format_instruction(example):\n    \"\"\"Format example into Alpaca template for training.\"\"\"\n    text = ALPACA_TEMPLATE.format(\n        instruction=example['instruction'],\n        input=example['input'],\n        output=example['output']\n    )\n    return {\"text\": text}\n\n# Create HuggingFace datasets\ntrain_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in train_data])\nval_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in val_data])\ntest_dataset = Dataset.from_list([{k: v for k, v in d.items() if k in ['instruction', 'input', 'output']} for d in test_data])\n\n# Apply formatting\ntrain_dataset = train_dataset.map(format_instruction)\nval_dataset = val_dataset.map(format_instruction)\ntest_dataset = test_dataset.map(format_instruction)\n\nprint(f\"✓ Prepared training dataset: {len(train_dataset)} examples\")\nprint(f\"✓ Prepared validation dataset: {len(val_dataset)} examples\")\nprint(f\"✓ Prepared test dataset: {len(test_dataset)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:31:37.913128Z","iopub.execute_input":"2026-02-16T14:31:37.913924Z","iopub.status.idle":"2026-02-16T14:31:38.271187Z","shell.execute_reply.started":"2026-02-16T14:31:37.913894Z","shell.execute_reply":"2026-02-16T14:31:38.270487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3546fcfac3784be5bf1a82d34ca94290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec91350da27a498cbf5397277599a24c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd7628af23b45cfa293d31cfa4de4ab"}},"metadata":{}},{"name":"stdout","text":"✓ Prepared training dataset: 4500 examples\n✓ Prepared validation dataset: 250 examples\n✓ Prepared test dataset: 250 examples\n","output_type":"stream"}],"execution_count":34},{"id":"03592b98","cell_type":"code","source":"# Training arguments\noutput_dir = \"/kaggle/working/\"\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n    per_device_eval_batch_size=4,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=500,\n    eval_steps=500,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_total_limit=3,\n    fp16=False,  # Use bf16 if supported\n    bf16=True,   # Better for training stability\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",  # Change to \"wandb\" if you want to use Weights & Biases\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\nprint(\"Training configuration:\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Batch size (per device): {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")\nprint(f\"  - LR scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"  - Warmup steps: {training_args.warmup_steps}\")\nprint(f\"  - Optimizer: {training_args.optim}\")\nprint(f\"  - Mixed precision: bf16={training_args.bf16}\")\nprint(f\"  - Gradient checkpointing: {training_args.gradient_checkpointing}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:39:44.543574Z","iopub.execute_input":"2026-02-16T14:39:44.544413Z","iopub.status.idle":"2026-02-16T14:39:44.586294Z","shell.execute_reply.started":"2026-02-16T14:39:44.544382Z","shell.execute_reply":"2026-02-16T14:39:44.585480Z"}},"outputs":[{"name":"stdout","text":"Training configuration:\n  - Epochs: 3\n  - Batch size (per device): 4\n  - Gradient accumulation steps: 4\n  - Effective batch size: 16\n  - Learning rate: 0.0002\n  - LR scheduler: SchedulerType.COSINE\n  - Warmup steps: 100\n  - Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n  - Mixed precision: bf16=True\n  - Gradient checkpointing: True\n","output_type":"stream"}],"execution_count":40},{"id":"8e3f70e6","cell_type":"code","source":"# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,                   \n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer,\n    args=sft_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:52:52.647634Z","iopub.execute_input":"2026-02-16T14:52:52.648368Z","iopub.status.idle":"2026-02-16T14:52:55.872101Z","shell.execute_reply.started":"2026-02-16T14:52:52.648337Z","shell.execute_reply":"2026-02-16T14:52:55.871362Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532d3a5d99b04a4ca448960873050225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c098f78dc1c4f1a858d66eec4f6a000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d3a70b730746fa82ecc7fd1e596134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d6fdf27054f4292a388f9ab5b76ffe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9e9aff48894479b7e5470e6bc2f6cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944c8f4d2b2243e0875bb35d09734244"}},"metadata":{}}],"execution_count":50},{"id":"6bd6854b","cell_type":"code","source":"# Start training\nprint(\"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\n# Train the model\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ TRAINING COMPLETE!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:53:10.647854Z","iopub.execute_input":"2026-02-16T14:53:10.648165Z","iopub.status.idle":"2026-02-16T14:53:11.167908Z","shell.execute_reply.started":"2026-02-16T14:53:10.648138Z","shell.execute_reply":"2026-02-16T14:53:11.167032Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSTARTING TRAINING\n============================================================\n\nThis will take approximately 1-2 hours on a T4 GPU...\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3940590382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m                 \u001b[0;31m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp8_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFP8BackendType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSAMP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_msamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1559\u001b[0;31m             result = tuple(\n\u001b[0m\u001b[1;32m   1560\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_msamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m             result = tuple(\n\u001b[0;32m-> 1560\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m             )\n\u001b[1;32m   1562\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m_prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1400\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                     \u001b[0;31m# if on the first device (GPU 0) we don't care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_device_index\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m   1793\u001b[0m                             \u001b[0;34m\"You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                             \u001b[0;34m\"you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`"],"ename":"ValueError","evalue":"You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`","output_type":"error"}],"execution_count":51},{"id":"f0bf6827","cell_type":"code","source":"# Save the final model\nfinal_model_path = \"../models/final/gemma-2b-financial-qa-lora\"\ntrainer.model.save_pretrained(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\nprint(f\"✓ Model saved to: {final_model_path}\")\nprint(\"\\nThe LoRA adapters have been saved and can be merged with the base model later.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.187693Z","iopub.status.idle":"2026-02-16T14:25:04.187992Z","shell.execute_reply.started":"2026-02-16T14:25:04.187854Z","shell.execute_reply":"2026-02-16T14:25:04.187877Z"}},"outputs":[],"execution_count":null},{"id":"95d961db","cell_type":"markdown","source":"---\n## 5. Inference & Evaluation\n\nTest the fine-tuned model with sample questions and evaluate on the test set.","metadata":{}},{"id":"c6904111","cell_type":"code","source":"# Inference function\ndef generate_response(instruction, input_context, max_new_tokens=256, temperature=0.7, top_p=0.9):\n    \"\"\"\n    Generate response for a given instruction and context.\n    \n    Args:\n        instruction: The question to answer\n        input_context: The context from 10-K filing\n        max_new_tokens: Maximum tokens to generate\n        temperature: Sampling temperature (higher = more creative)\n        top_p: Nucleus sampling parameter\n    \n    Returns:\n        Generated response text\n    \"\"\"\n    # Format prompt\n    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_context}\n\n### Response:\n\"\"\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # Decode\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the response part\n    response = full_output.split(\"### Response:\")[-1].strip()\n    \n    return response\n\nprint(\"✓ Inference function ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.189174Z","iopub.status.idle":"2026-02-16T14:25:04.189454Z","shell.execute_reply.started":"2026-02-16T14:25:04.189319Z","shell.execute_reply":"2026-02-16T14:25:04.189341Z"}},"outputs":[],"execution_count":null},{"id":"6179d443","cell_type":"code","source":"# Test with sample examples from test set\nprint(\"=\"*60)\nprint(\"INFERENCE EXAMPLES\")\nprint(\"=\"*60)\n\nnum_examples = 3\nfor i in range(min(num_examples, len(test_data))):\n    example = test_data[i]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"EXAMPLE {i+1} - {example['ticker']}\")\n    print(f\"{'='*60}\")\n    \n    print(f\"\\nQuestion:\\n{example['instruction']}\")\n    print(f\"\\nContext (excerpt):\\n{example['input'][:200]}...\")\n    \n    # Generate prediction\n    prediction = generate_response(\n        example['instruction'], \n        example['input'],\n        max_new_tokens=200,\n        temperature=0.7\n    )\n    \n    print(f\"\\n{'─'*60}\")\n    print(f\"MODEL PREDICTION:\\n{prediction}\")\n    print(f\"\\n{'─'*60}\")\n    print(f\"GROUND TRUTH:\\n{example['output']}\")\n    print(f\"{'─'*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.190527Z","iopub.status.idle":"2026-02-16T14:25:04.190845Z","shell.execute_reply.started":"2026-02-16T14:25:04.190675Z","shell.execute_reply":"2026-02-16T14:25:04.190702Z"}},"outputs":[],"execution_count":null},{"id":"93c041be","cell_type":"code","source":"# Evaluate on test set\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING ON TEST SET\")\nprint(\"=\"*60)\n\n# Load ROUGE metric\nrouge = evaluate.load('rouge')\n\npredictions = []\nreferences = []\n\nprint(f\"\\nGenerating predictions for {len(test_data)} test examples...\\n\")\n\nfor example in tqdm(test_data[:100], desc=\"Evaluating\"):  # Evaluate on first 100 for speed\n    pred = generate_response(\n        example['instruction'], \n        example['input'],\n        max_new_tokens=200,\n        temperature=0.7\n    )\n    predictions.append(pred)\n    references.append(example['output'])\n\n# Calculate ROUGE scores\nrouge_scores = rouge.compute(predictions=predictions, references=references)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"\\nROUGE Scores (on {len(predictions)} test examples):\")\nprint(f\"  - ROUGE-1: {rouge_scores['rouge1']:.4f}\")\nprint(f\"  - ROUGE-2: {rouge_scores['rouge2']:.4f}\")\nprint(f\"  - ROUGE-L: {rouge_scores['rougeL']:.4f}\")\nprint(f\"  - ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.192027Z","iopub.status.idle":"2026-02-16T14:25:04.192347Z","shell.execute_reply.started":"2026-02-16T14:25:04.192174Z","shell.execute_reply":"2026-02-16T14:25:04.192199Z"}},"outputs":[],"execution_count":null},{"id":"881786ac","cell_type":"code","source":"# Save evaluation results\nresults = {\n    \"model\": MODEL_NAME,\n    \"method\": \"QLoRA\",\n    \"lora_config\": {\n        \"r\": peft_config.r,\n        \"alpha\": peft_config.lora_alpha,\n        \"dropout\": peft_config.lora_dropout,\n        \"target_modules\": peft_config.target_modules\n    },\n    \"training_examples\": len(train_data),\n    \"eval_examples\": len(predictions),\n    \"rouge_scores\": rouge_scores,\n    \"sample_predictions\": [\n        {\n            \"instruction\": test_data[i]['instruction'],\n            \"prediction\": predictions[i],\n            \"reference\": references[i]\n        }\n        for i in range(min(5, len(predictions)))\n    ]\n}\n\nresults_path = \"../outputs/results/evaluation_results.json\"\nos.makedirs(os.path.dirname(results_path), exist_ok=True)\n\nwith open(results_path, 'w', encoding='utf-8') as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\n✓ Evaluation results saved to: {results_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.193511Z","iopub.status.idle":"2026-02-16T14:25:04.193821Z","shell.execute_reply.started":"2026-02-16T14:25:04.193669Z","shell.execute_reply":"2026-02-16T14:25:04.193697Z"}},"outputs":[],"execution_count":null},{"id":"63297a30","cell_type":"markdown","source":"---\n## 6. Interactive Testing (Optional)\n\nTest the model with custom questions.","metadata":{}},{"id":"7e81968a","cell_type":"code","source":"# Interactive testing - modify these to test your own questions\ncustom_instruction = \"What was the company's total revenue?\"\ncustom_context = \"\"\"The company reported strong financial performance for fiscal year 2023. \nTotal revenue increased by 15% year-over-year to reach $26.97 billion. \nThis growth was primarily driven by increased demand for our products and services.\"\"\"\n\nprint(\"=\"*60)\nprint(\"CUSTOM QUESTION TEST\")\nprint(\"=\"*60)\nprint(f\"\\nQuestion: {custom_instruction}\")\nprint(f\"\\nContext: {custom_context}\")\n\nresponse = generate_response(custom_instruction, custom_context, max_new_tokens=150)\n\nprint(f\"\\nModel Response:\\n{response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.195057Z","iopub.status.idle":"2026-02-16T14:25:04.195420Z","shell.execute_reply.started":"2026-02-16T14:25:04.195234Z","shell.execute_reply":"2026-02-16T14:25:04.195258Z"}},"outputs":[],"execution_count":null},{"id":"36a373bc","cell_type":"markdown","source":"---\n## 7. Export Model (Optional)\n\nUpload to HuggingFace Hub or download locally.","metadata":{}},{"id":"2a454205","cell_type":"code","source":"# Optional: Push to HuggingFace Hub\n# Uncomment and configure if you want to share your model\n\n# from huggingface_hub import HfApi\n# \n# model_id = \"your-username/gemma-2b-financial-qa-lora\"\n# trainer.model.push_to_hub(model_id)\n# tokenizer.push_to_hub(model_id)\n# \n# print(f\"✓ Model pushed to HuggingFace Hub: {model_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.196590Z","iopub.status.idle":"2026-02-16T14:25:04.196921Z","shell.execute_reply.started":"2026-02-16T14:25:04.196788Z","shell.execute_reply":"2026-02-16T14:25:04.196809Z"}},"outputs":[],"execution_count":null},{"id":"611ecaf2","cell_type":"code","source":"# Create a downloadable archive (for local use)\nimport shutil\n\narchive_path = \"../gemma-2b-financial-qa-model\"\nif os.path.exists(archive_path + \".zip\"):\n    os.remove(archive_path + \".zip\")\n\nshutil.make_archive(archive_path, 'zip', final_model_path)\nprint(f\"✓ Model archived to: {archive_path}.zip\")\nprint(\"\\nYou can download this file and use it locally or upload to HuggingFace.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:25:04.197743Z","iopub.status.idle":"2026-02-16T14:25:04.198317Z","shell.execute_reply.started":"2026-02-16T14:25:04.198180Z","shell.execute_reply":"2026-02-16T14:25:04.198198Z"}},"outputs":[],"execution_count":null},{"id":"faee8126","cell_type":"markdown","source":"---\n## Summary\n\nYou have successfully:\n1. ✓ Preprocessed 5,000 financial Q&A examples into Alpaca format\n2. ✓ Fine-tuned Gemma-2B using QLoRA (4-bit quantization)\n3. ✓ Evaluated the model on test data using ROUGE metrics\n4. ✓ Saved the fine-tuned LoRA adapters\n\n**Next Steps:**\n- Experiment with different hyperparameters (learning rate, LoRA rank, etc.)\n- Try fine-tuning on the full dataset (7,000+ examples)\n- Test with real-world financial questions\n- Integrate into a RAG (Retrieval-Augmented Generation) pipeline\n- Deploy as an API using FastAPI or Gradio\n\n**Model Usage:**\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load base model and LoRA adapters\nbase_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\nmodel = PeftModel.from_pretrained(base_model, \"path/to/lora/adapters\")\ntokenizer = AutoTokenizer.from_pretrained(\"path/to/lora/adapters\")\n```","metadata":{}}]}